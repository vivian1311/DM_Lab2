{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81989543",
   "metadata": {},
   "source": [
    "# Part II: Kaggle Competition (30%)\n",
    "Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "   - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "   - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "\n",
    "Submit your last submission **BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adcc5a",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c29e9",
   "metadata": {},
   "source": [
    "To load the data efficiently, I first use this file to preprocess the data and save them as a `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5ce177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c05f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_identification = pd.read_csv('part2_data/data_identification.csv')\n",
    "df_tweets = pd.read_json('part2_data/tweets_DM.json', lines = True)\n",
    "df_emotion = pd.read_csv('part2_data/emotion.csv')\n",
    "# df_tweets = pd.read_pickle(\"part2_data/cleaned_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138fde7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identification shape: (1867535, 2)\n",
      "tweets shape: (1867535, 5)\n",
      "emotion shape: (1455563, 2)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of data\n",
    "print(\"identification shape:\", df_identification.shape)\n",
    "print(\"tweets shape:\", df_tweets.shape)\n",
    "print(\"emotion shape:\", df_emotion.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfcc70",
   "metadata": {},
   "source": [
    "## 1.1 Clean the tweets data\n",
    "We can find that in `_source` column, there are the main informations we need. Therefore, let's split these informations out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ce4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source info\n",
    "def get_hashtags(t):\n",
    "    r = []\n",
    "    hashtags = t['tweet']['hashtags']\n",
    "    for i in range(len(hashtags)):\n",
    "        r.append(hashtags[i])\n",
    "    return r\n",
    "\n",
    "def get_id(t):\n",
    "    return t['tweet']['tweet_id']\n",
    "\n",
    "def get_text(t):\n",
    "    return t['tweet']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e58a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hashtags, tweet_id, and text from `_source`\n",
    "df_tweets['hashtags'] = df_tweets['_source'].apply(lambda x: get_hashtags(x))\n",
    "df_tweets['tweet_id'] = df_tweets['_source'].apply(lambda x: get_id(x))\n",
    "df_tweets['text'] = df_tweets['_source'].apply(lambda x: get_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e8783",
   "metadata": {},
   "source": [
    "After getting the informations from `_source`, we now can drop the useless columns from the dataframe. By using `groupby` to check the columns, we found that `_type` and `_index` store the same info. in each row. Therefore. we can drop them to get cleaner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4213539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_type\n",
      "tweets    1867535\n",
      "Name: _source, dtype: int64\n",
      "_index\n",
      "hashtag_tweets    1867535\n",
      "Name: _source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the labels\n",
    "print(df_tweets.groupby(['_type']).count()['_source'])\n",
    "print(df_tweets.groupby(['_index']).count()['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014dd3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                                hashtags  \n",
       "0                             [Snapchat]  \n",
       "1          [freepress, TrumpLegacy, CNN]  \n",
       "2                           [bibleverse]  \n",
       "3                                     []  \n",
       "4                                     []  \n",
       "...                                  ...  \n",
       "1867530  [mixedfeeling, butimTHATperson]  \n",
       "1867531                               []  \n",
       "1867532                               []  \n",
       "1867533                               []  \n",
       "1867534                    [Sundayvibes]  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets.drop(columns=['_score', '_crawldate', '_type', '_index', '_source'])\n",
    "df_tweets = df_tweets.reindex(columns=['tweet_id','text','hashtags'])\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5ab2f",
   "metadata": {},
   "source": [
    "## 1.2 Concatenate the data\n",
    "To juxtapose the text, label, and training/testing set, I concatenate them on `tweet_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f45715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                                hashtags       emotion identification  \n",
       "0                             [Snapchat]  anticipation          train  \n",
       "1          [freepress, TrumpLegacy, CNN]       sadness          train  \n",
       "2                           [bibleverse]           NaN           test  \n",
       "3                                     []          fear          train  \n",
       "4                                     []           NaN           test  \n",
       "...                                  ...           ...            ...  \n",
       "1867530  [mixedfeeling, butimTHATperson]           NaN           test  \n",
       "1867531                               []           NaN           test  \n",
       "1867532                               []           NaN           test  \n",
       "1867533                               []           joy          train  \n",
       "1867534                    [Sundayvibes]           joy          train  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_tweets.merge(df_emotion, on = 'tweet_id', how='left')\n",
    "df = df.merge(df_identification, on = 'tweet_id', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1102c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def tokenize_text(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Tokenize text using the nltk library\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for d in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(d, language='english'):\n",
    "            # filters here\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bb5ba",
   "metadata": {},
   "source": [
    "In case that we need to remove the stopwords while preprcossing, I add the column with text removed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d085e3a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vivian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_eng = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e8a64c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, ``, add, me, on, #, Snapch...</td>\n",
       "      <td>People post `` add Snapchat '' must dehydrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@, brianklaas, As, we, see, ,, Trump, is, dan...</td>\n",
       "      <td>brianklaas see Trump dangerous freepress aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[Confident, of, your, obedience, ,, I, write, ...</td>\n",
       "      <td>Confident obedience write knowing even ask Phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, &lt;, LH, &gt;]</td>\n",
       "      <td>ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[``, Trust, is, not, the, same, as, faith, ., ...</td>\n",
       "      <td>`` Trust faith friend someone trust Putting fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[When, you, buy, the, last, 2, tickets, remain...</td>\n",
       "      <td>buy last 2 tickets remaining show sell .. mixe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[I, swear, all, this, hard, work, gone, pay, o...</td>\n",
       "      <td>swear hard work gone pay one dayðŸ˜ˆðŸ’°ðŸ’¸ LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, Parcel2Go, no, card, left, when, I, was, n...</td>\n",
       "      <td>Parcel2Go card left n't idea get parcel LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[Ah, ,, corporate, life, ,, where, you, can, d...</td>\n",
       "      <td>Ah corporate life date LH using relative anach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[Blessed, to, be, living, #, Sundayvibes, &lt;, L...</td>\n",
       "      <td>Blessed living Sundayvibes LH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                                hashtags       emotion identification  \\\n",
       "0                             [Snapchat]  anticipation          train   \n",
       "1          [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "2                           [bibleverse]           NaN           test   \n",
       "3                                     []          fear          train   \n",
       "4                                     []           NaN           test   \n",
       "...                                  ...           ...            ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]           NaN           test   \n",
       "1867531                               []           NaN           test   \n",
       "1867532                               []           NaN           test   \n",
       "1867533                               []           joy          train   \n",
       "1867534                    [Sundayvibes]           joy          train   \n",
       "\n",
       "                                                  unigrams  \\\n",
       "0        [People, who, post, ``, add, me, on, #, Snapch...   \n",
       "1        [@, brianklaas, As, we, see, ,, Trump, is, dan...   \n",
       "2        [Confident, of, your, obedience, ,, I, write, ...   \n",
       "3          [Now, ISSA, is, stalking, Tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, <, LH, >]   \n",
       "4        [``, Trust, is, not, the, same, as, faith, ., ...   \n",
       "...                                                    ...   \n",
       "1867530  [When, you, buy, the, last, 2, tickets, remain...   \n",
       "1867531  [I, swear, all, this, hard, work, gone, pay, o...   \n",
       "1867532  [@, Parcel2Go, no, card, left, when, I, was, n...   \n",
       "1867533  [Ah, ,, corporate, life, ,, where, you, can, d...   \n",
       "1867534  [Blessed, to, be, living, #, Sundayvibes, <, L...   \n",
       "\n",
       "                                          remove_stopwords  \n",
       "0        People post `` add Snapchat '' must dehydrated...  \n",
       "1        brianklaas see Trump dangerous freepress aroun...  \n",
       "2        Confident obedience write knowing even ask Phi...  \n",
       "3                               ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH  \n",
       "4        `` Trust faith friend someone trust Putting fa...  \n",
       "...                                                    ...  \n",
       "1867530  buy last 2 tickets remaining show sell .. mixe...  \n",
       "1867531             swear hard work gone pay one dayðŸ˜ˆðŸ’°ðŸ’¸ LH  \n",
       "1867532         Parcel2Go card left n't idea get parcel LH  \n",
       "1867533  Ah corporate life date LH using relative anach...  \n",
       "1867534                      Blessed living Sundayvibes LH  \n",
       "\n",
       "[1867535 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords and punctuation\n",
    "import string\n",
    "df['unigrams'] = df['text'].apply(lambda x: tokenize_text(x))\n",
    "df['remove_stopwords'] = df['unigrams'].apply(lambda x: [item for item in x if (item.lower() not in stopwords_eng and item.lower() not in string.punctuation)])\n",
    "# turn into string and use' ' to seperate each terms\n",
    "df['remove_stopwords'] = [' '.join(map(str, l)) for l in df['remove_stopwords']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca64074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "df.to_pickle(\"part2_data/cleaned_tweets.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3b50ee",
   "metadata": {},
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b3107",
   "metadata": {},
   "source": [
    "To improvement the performance, I implement the pre-trained model - Bert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ef66d",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68b2ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 11:11:44.989756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-16 11:11:45.288186: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-16 11:11:45.288198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-16 11:11:45.325782: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-16 11:11:46.072223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-16 11:11:46.072311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-16 11:11:46.072316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 4.2.0\n",
      "tensorflow: 2.10.0\n",
      "keras: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"gensim: \" + gensim.__version__)\n",
    "print(\"tensorflow: \" + tf.__version__)\n",
    "print(\"keras: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7208e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a380715",
   "metadata": {},
   "source": [
    "Since the pre-train language model requires lots of computation resource, I use GPU to run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c135d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7cfd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e02e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f42e1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_pickle(\"part2_data/cleaned_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cff884",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466ce082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, ``, add, me, on, #, Snapch...</td>\n",
       "      <td>People post `` add Snapchat '' must dehydrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@, brianklaas, As, we, see, ,, Trump, is, dan...</td>\n",
       "      <td>brianklaas see Trump dangerous freepress aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, &lt;, LH, &gt;]</td>\n",
       "      <td>ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@, RISKshow, @, TheKevinAllison, Thx, for, th...</td>\n",
       "      <td>RISKshow TheKevinAllison Thx BEST TIME tonight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus, ...</td>\n",
       "      <td>Still waiting supplies Liscus LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[I, 'm, SO, HAPPY, !, !, !, #, NoWonder, the, ...</td>\n",
       "      <td>'m HAPPY NoWonder name show Happy HappySYFY SY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[In, every, circumtance, I, 'd, like, to, be, ...</td>\n",
       "      <td>every circumtance 'd like thankful Almighty Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[there, 's, currently, two, girls, walking, ar...</td>\n",
       "      <td>'s currently two girls walking around library ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[Ah, ,, corporate, life, ,, where, you, can, d...</td>\n",
       "      <td>Ah corporate life date LH using relative anach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[Blessed, to, be, living, #, Sundayvibes, &lt;, L...</td>\n",
       "      <td>Blessed living Sundayvibes LH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1867526  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1867527  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1867528  0x2cbca6  there's currently two girls walking around the...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                              hashtags       emotion identification  \\\n",
       "0                           [Snapchat]  anticipation          train   \n",
       "1        [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                                   []          fear          train   \n",
       "5            [authentic, LaughOutLoud]           joy          train   \n",
       "6                                   []  anticipation          train   \n",
       "...                                ...           ...            ...   \n",
       "1867526              [NoWonder, Happy]           joy          train   \n",
       "1867527                             []           joy          train   \n",
       "1867528                     [blessyou]           joy          train   \n",
       "1867533                             []           joy          train   \n",
       "1867534                  [Sundayvibes]           joy          train   \n",
       "\n",
       "                                                  unigrams  \\\n",
       "0        [People, who, post, ``, add, me, on, #, Snapch...   \n",
       "1        [@, brianklaas, As, we, see, ,, Trump, is, dan...   \n",
       "3          [Now, ISSA, is, stalking, Tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, <, LH, >]   \n",
       "5        [@, RISKshow, @, TheKevinAllison, Thx, for, th...   \n",
       "6        [Still, waiting, on, those, supplies, Liscus, ...   \n",
       "...                                                    ...   \n",
       "1867526  [I, 'm, SO, HAPPY, !, !, !, #, NoWonder, the, ...   \n",
       "1867527  [In, every, circumtance, I, 'd, like, to, be, ...   \n",
       "1867528  [there, 's, currently, two, girls, walking, ar...   \n",
       "1867533  [Ah, ,, corporate, life, ,, where, you, can, d...   \n",
       "1867534  [Blessed, to, be, living, #, Sundayvibes, <, L...   \n",
       "\n",
       "                                          remove_stopwords  \n",
       "0        People post `` add Snapchat '' must dehydrated...  \n",
       "1        brianklaas see Trump dangerous freepress aroun...  \n",
       "3                               ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH  \n",
       "5        RISKshow TheKevinAllison Thx BEST TIME tonight...  \n",
       "6                         Still waiting supplies Liscus LH  \n",
       "...                                                    ...  \n",
       "1867526  'm HAPPY NoWonder name show Happy HappySYFY SY...  \n",
       "1867527  every circumtance 'd like thankful Almighty Je...  \n",
       "1867528  's currently two girls walking around library ...  \n",
       "1867533  Ah corporate life date LH using relative anach...  \n",
       "1867534                      Blessed living Sundayvibes LH  \n",
       "\n",
       "[1455563 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.loc[df.identification == 'train']\n",
    "df_test = df.loc[df.identification == 'test']\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebe27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "anger            39867\n",
      "anticipation    248935\n",
      "disgust         139101\n",
      "fear             63999\n",
      "joy             516017\n",
      "sadness         193437\n",
      "surprise         48729\n",
      "trust           205478\n",
      "Name: text, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'category')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAIJCAYAAABJOFKvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWC0lEQVR4nO3de1gUZf8/8PfKYQWEBUXAVRRNRRDQgkS0QkVE81BajxVFYqaWphGQh8oT5VnR1DItE09JPRo9lYaQpkYKIoJKKqaCYICYclBUQLh/f/hjvq4gioEDO+/Xde11tTOf3fnMkuybmfueUQkhBIiIiIgUqIncDRARERHJhUGIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYioEYqIiIBKpbrnY+/evY+8p507d2L27NnVrnNwcEBgYOAj7edBZWRkQKVSISIiQu5WGpXs7GzMnj0bKSkpcrdC9K8Yyt0AET289evXo0uXLlWWOzs7P/Jedu7cic8++6zaMBQVFQULC4tH3hPVn+zsbMyZMwcODg7o3r273O0QPTQGIaJGzMXFBR4eHnK3cV+PP/643C0QEVWLp8aI9JxKpcI777yD9evXw9HRESYmJvDw8EB8fDyEEFi8eDHat2+PZs2aoV+/fjhz5kyV9/j666/RrVs3NG3aFM2bN8fw4cNx8uRJaX1gYCA+++wzaXuVj4yMDADVnxrLzMzEa6+9BhsbG6jVajg5OWHp0qWoqKiQaipPWy1ZsgTh4eFSn15eXoiPj3+g/f/7778xbtw42Nvbw9jYGFqtFi+++CIuXrx4z9ecOXMGo0ePRqdOnWBqaorWrVtj6NChOH78uE5dRUUFPvnkE+lztbS0hJubGz799FOp5tKlS9L21Wo1WrZsid69e+PXX3+9b++nTp3CK6+8AltbW6jVarRt2xavv/46SkpKpJrU1FQ899xzsLKyQtOmTdG9e3ds2LBB530qT6VW/jwq7d27t8qp1D59+sDFxQWJiYl4+umnYWpqig4dOmDBggXSz2bv3r148sknAQCjR4+Wft73OjVK1JDxiBBRI1ZeXo5bt27pLFOpVDAwMNBZ9vPPPyM5ORkLFiyASqXC1KlTMXjwYIwaNQrnzp3DqlWrUFhYiODgYLzwwgtISUmBSqUCAMyfPx8ffPABXnnlFcyfPx+XL1/G7Nmz4eXlhcTERHTq1AkzZsxAcXExtm3bhoMHD0rbbdWqVbV9X7p0Cb169UJpaSk+/vhjODg44Oeff0ZoaCjOnj2Lzz//XKf+s88+Q5cuXbB8+XIAwIwZM/Dss88iPT0dGo3mnp/P33//jSeffBJlZWX44IMP4ObmhsuXL2PXrl3Iz8+Hra1tta/Lzs5GixYtsGDBArRs2RJXrlzBhg0b4OnpieTkZDg6OgIAFi1ahNmzZ+Ojjz7CM888g7KyMpw6dQoFBQXSewUEBODIkSOYO3cuOnfujIKCAhw5cgSXL1++Z98AcPToUTz11FOwtrZGWFgYOnXqhJycHPz4448oLS2FWq1GWloaevXqBRsbG6xYsQItWrTA5s2bERgYiIsXL2LKlCk1buNecnNz8eqrryIkJASzZs1CVFQUpk+fDq1Wi9dffx1PPPEE1q9fj9GjR+Ojjz7C4MGDAQBt2rR5qO0RyUoQUaOzfv16AaDah4GBgU4tAGFnZyeuXbsmLfvhhx8EANG9e3dRUVEhLV++fLkAII4dOyaEECI/P1+YmJiIZ599Vuc9MzMzhVqtFv7+/tKyiRMninv9SmnXrp0YNWqU9HzatGkCgEhISNCpe/vtt4VKpRJpaWlCCCHS09MFAOHq6ipu3bol1R06dEgAEFu3bq3xc3rjjTeEkZGROHHixD1rKrexfv36e9bcunVLlJaWik6dOon33ntPWj5kyBDRvXv3Gnto1qyZCAoKqrGmOv369ROWlpYiLy/vnjUvv/yyUKvVIjMzU2f5oEGDhKmpqSgoKBBC/N//L+np6Tp1v/32mwAgfvvtN2mZt7d3tT8bZ2dn4efnJz1PTEy87+dG1Bjw1BhRI7Zx40YkJibqPBISEqrU9e3bF2ZmZtJzJycnAMCgQYOkIz93Lj9//jwA4ODBg7hx40aV01r29vbo168fdu/e/VB979mzB87OzujRo4fO8sDAQAghsGfPHp3lgwcP1jnK5ebmptPnvfzyyy/o27evtF8P6tatW5g3bx6cnZ1hbGwMQ0NDGBsb46+//tI5JdijRw8cPXoUEyZMwK5du1BUVFTlvXr06IGIiAh88skniI+PR1lZ2X23f/36dezbtw8jR45Ey5Yt71m3Z88e+Pj4wN7eXmd5YGAgrl+/rnN0rjbs7Oyq/Gzc3Nzu+3kTNUYMQkSNmJOTEzw8PHQe7u7uVeqaN2+u89zY2LjG5Tdv3gQA6fRNdae4tFrtfU/v3Mvly5fv+Z53brdSixYtdJ6r1WoAwI0bN2rczqVLlx7qdE1wcDBmzJiB559/Hj/99BMSEhKQmJiIbt266Wxz+vTpWLJkCeLj4zFo0CC0aNECPj4+OHz4sFTz7bffYtSoUfjqq6/g5eWF5s2b4/XXX0dubu49t5+fn4/y8vL79l7bz/FB3f15A7c/8/t93kSNEYMQEd1T5RdiTk5OlXXZ2dmwtrZ+6Pe913sCeOj3vVvLli1x4cKFWr9u8+bNeP311zFv3jz4+fmhR48e8PDwwD///KNTZ2hoiODgYBw5cgRXrlzB1q1bkZWVBT8/P1y/fl3al+XLlyMjIwPnz5/H/Pnz8f3339d4XaXmzZvDwMDgvr0/6OfYtGlTANAZZA2gyv4QKRGDEBHdk5eXF0xMTLB582ad5RcuXJBOy1R60KM0AODj44MTJ07gyJEjOss3btwIlUqFvn371kH3t0/9/fbbb0hLS6vV61QqlbQ/lXbs2IG///77nq+xtLTEiy++iIkTJ+LKlStVZmgBQNu2bfHOO+/A19e3yr7fycTEBN7e3vjvf/9bY1jx8fHBnj17pOBTaePGjTA1NUXPnj0B3J61BwDHjh3Tqfvxxx/v+d73U5ufN1FDxlljRI1YampqlVljAPDYY4/VOLbkQVlaWmLGjBn44IMP8Prrr+OVV17B5cuXMWfOHDRt2hSzZs2Sal1dXQEACxcuxKBBg2BgYAA3NzfpdNud3nvvPWzcuBGDBw9GWFgY2rVrhx07duDzzz/H22+/jc6dO//r3gEgLCwMv/zyC5555hl88MEHcHV1RUFBAaKjoxEcHFztxSgBYMiQIYiIiECXLl3g5uaGpKQkLF68uMqpqqFDh0rXcmrZsiXOnz+P5cuXo127dujUqRMKCwvRt29f+Pv7o0uXLjA3N0diYiKio6MxYsSIGnsPDw/HU089BU9PT0ybNg0dO3bExYsX8eOPP2LNmjUwNzfHrFmz8PPPP6Nv376YOXMmmjdvji1btmDHjh1YtGiRNKPuySefhKOjI0JDQ3Hr1i1YWVkhKioKcXFxD/3ZPvbYYzAxMcGWLVvg5OSEZs2aQavVSqfliBoNuUdrE1Ht1TRrDID48ssvpVoAYuLEiTqvr5wptXjxYp3llbOI/vvf/+os/+qrr4Sbm5swNjYWGo1GPPfcc+LPP//UqSkpKRFvvvmmaNmypVCpVDqzlO6eNSaEEOfPnxf+/v6iRYsWwsjISDg6OorFixeL8vLy+/ZZuV+zZs2672eVlZUl3njjDWFnZyeMjIyEVqsVI0eOFBcvXtTZxp2zn/Lz88WYMWOEjY2NMDU1FU899ZT4/fffhbe3t/D29pbqli5dKnr16iWsra2FsbGxaNu2rRgzZozIyMgQQghx8+ZN8dZbbwk3NzdhYWEhTExMhKOjo5g1a5YoLi6+b+8nTpwQ//nPf0SLFi2k9w8MDBQ3b96Uao4fPy6GDh0qNBqNMDY2Ft26dat2Jtfp06fFgAEDhIWFhWjZsqWYNGmS2LFjR7Wzxrp27Vrl9aNGjRLt2rXTWbZ161bRpUsXYWRk9MA/D6KGRiWEEI8+fhERERHJj2OEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsXhBxfuoqKhAdnY2zM3NdW5OSURERA2XEAJXr16FVqtFkyb3Pu7DIHQf2dnZVe7sTERERI1DVlZWjTcwZhC6D3NzcwC3P0gLCwuZuyEiIqIHUVRUBHt7e+l7/F4YhO6j8nSYhYUFgxAREVEjc79hLRwsTURERIrFIERERESKxSBEREREisUgRERERIrFIERERESKxSBEREREisUgRERERIrFIERERESKxSBEREREisUgRERERIrFIERERESKxSBEREREisUgRERERIrFIERERESKZSh3A/rOYdoO2badsWCwbNsmIiJqDHhEiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFKtWQWj27NlQqVQ6Dzs7O2m9EAKzZ8+GVquFiYkJ+vTpgz///FPnPUpKSjBp0iRYW1vDzMwMw4YNw4ULF3Rq8vPzERAQAI1GA41Gg4CAABQUFOjUZGZmYujQoTAzM4O1tTUmT56M0tJSnZrjx4/D29sbJiYmaN26NcLCwiCEqM0uExERkR6r9RGhrl27IicnR3ocP35cWrdo0SKEh4dj1apVSExMhJ2dHXx9fXH16lWpJigoCFFRUYiMjERcXByuXbuGIUOGoLy8XKrx9/dHSkoKoqOjER0djZSUFAQEBEjry8vLMXjwYBQXFyMuLg6RkZHYvn07QkJCpJqioiL4+vpCq9UiMTERK1euxJIlSxAeHl7rD4mIiIj0U60vqGhoaKhzFKiSEALLly/Hhx9+iBEjRgAANmzYAFtbW3zzzTcYP348CgsLsW7dOmzatAn9+/cHAGzevBn29vb49ddf4efnh5MnTyI6Ohrx8fHw9PQEAHz55Zfw8vJCWloaHB0dERMTgxMnTiArKwtarRYAsHTpUgQGBmLu3LmwsLDAli1bcPPmTURERECtVsPFxQWnT59GeHg4goODoVKpHvpDIyIiIv1Q6yNCf/31F7RaLdq3b4+XX34Z586dAwCkp6cjNzcXAwYMkGrVajW8vb1x4MABAEBSUhLKysp0arRaLVxcXKSagwcPQqPRSCEIAHr27AmNRqNT4+LiIoUgAPDz80NJSQmSkpKkGm9vb6jVap2a7OxsZGRk3HP/SkpKUFRUpPMgIiIi/VSrIOTp6YmNGzdi165d+PLLL5Gbm4tevXrh8uXLyM3NBQDY2trqvMbW1lZal5ubC2NjY1hZWdVYY2NjU2XbNjY2OjV3b8fKygrGxsY11lQ+r6ypzvz586WxSRqNBvb29jV/KERERNRo1SoIDRo0CC+88AJcXV3Rv39/7Nhx+z5aGzZskGruPuUkhLjvaai7a6qrr4uayoHSNfUzffp0FBYWSo+srKwaeyciIqLG619NnzczM4Orqyv++usvadzQ3Udb8vLypCMxdnZ2KC0tRX5+fo01Fy9erLKtS5cu6dTcvZ38/HyUlZXVWJOXlweg6lGrO6nValhYWOg8iIiISD/9qyBUUlKCkydPolWrVmjfvj3s7OwQGxsrrS8tLcW+ffvQq1cvAIC7uzuMjIx0anJycpCamirVeHl5obCwEIcOHZJqEhISUFhYqFOTmpqKnJwcqSYmJgZqtRru7u5Szf79+3Wm1MfExECr1cLBweHf7DYRERHpiVoFodDQUOzbtw/p6elISEjAiy++iKKiIowaNQoqlQpBQUGYN28eoqKikJqaisDAQJiamsLf3x8AoNFoMGbMGISEhGD37t1ITk7Ga6+9Jp1qAwAnJycMHDgQY8eORXx8POLj4zF27FgMGTIEjo6OAIABAwbA2dkZAQEBSE5Oxu7duxEaGoqxY8dKR3D8/f2hVqsRGBiI1NRUREVFYd68eZwxRkRERJJaTZ+/cOECXnnlFfzzzz9o2bIlevbsifj4eLRr1w4AMGXKFNy4cQMTJkxAfn4+PD09ERMTA3Nzc+k9li1bBkNDQ4wcORI3btyAj48PIiIiYGBgINVs2bIFkydPlmaXDRs2DKtWrZLWGxgYYMeOHZgwYQJ69+4NExMT+Pv7Y8mSJVKNRqNBbGwsJk6cCA8PD1hZWSE4OBjBwcEP90kRERGR3lEJXmq5RkVFRdBoNCgsLHyo8UIO03bUQ1cPJmPBYNm2TUREJKcH/f7mvcaIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixGISIiIhIsf5VEJo/fz5UKhWCgoKkZUIIzJ49G1qtFiYmJujTpw/+/PNPndeVlJRg0qRJsLa2hpmZGYYNG4YLFy7o1OTn5yMgIAAajQYajQYBAQEoKCjQqcnMzMTQoUNhZmYGa2trTJ48GaWlpTo1x48fh7e3N0xMTNC6dWuEhYVBCPFvdpuIiIj0xEMHocTERKxduxZubm46yxctWoTw8HCsWrUKiYmJsLOzg6+vL65evSrVBAUFISoqCpGRkYiLi8O1a9cwZMgQlJeXSzX+/v5ISUlBdHQ0oqOjkZKSgoCAAGl9eXk5Bg8ejOLiYsTFxSEyMhLbt29HSEiIVFNUVARfX19otVokJiZi5cqVWLJkCcLDwx92t4mIiEiPqMRDHB65du0annjiCXz++ef45JNP0L17dyxfvhxCCGi1WgQFBWHq1KkAbh/9sbW1xcKFCzF+/HgUFhaiZcuW2LRpE1566SUAQHZ2Nuzt7bFz5074+fnh5MmTcHZ2Rnx8PDw9PQEA8fHx8PLywqlTp+Do6IhffvkFQ4YMQVZWFrRaLQAgMjISgYGByMvLg4WFBVavXo3p06fj4sWLUKvVAIAFCxZg5cqVuHDhAlQqVZV9KykpQUlJifS8qKgI9vb2KCwshIWFRW0/KjhM21Hr19SVjAWDZds2ERGRnIqKiqDRaO77/f1QR4QmTpyIwYMHo3///jrL09PTkZubiwEDBkjL1Go1vL29ceDAAQBAUlISysrKdGq0Wi1cXFykmoMHD0Kj0UghCAB69uwJjUajU+Pi4iKFIADw8/NDSUkJkpKSpBpvb28pBFXWZGdnIyMjo9p9mz9/vnQ6TqPRwN7e/mE+IiIiImoEah2EIiMjceTIEcyfP7/KutzcXACAra2tznJbW1tpXW5uLoyNjWFlZVVjjY2NTZX3t7Gx0am5eztWVlYwNjausabyeWXN3aZPn47CwkLpkZWVVW0dERERNX6GtSnOysrCu+++i5iYGDRt2vSedXefchJCVHsaqqaa6urroqbyTOC9+lGr1TpHkIiIiEh/1eqIUFJSEvLy8uDu7g5DQ0MYGhpi3759WLFiBQwNDe95tCUvL09aZ2dnh9LSUuTn59dYc/HixSrbv3Tpkk7N3dvJz89HWVlZjTV5eXkAqh61IiIiIuWpVRDy8fHB8ePHkZKSIj08PDzw6quvIiUlBR06dICdnR1iY2Ol15SWlmLfvn3o1asXAMDd3R1GRkY6NTk5OUhNTZVqvLy8UFhYiEOHDkk1CQkJKCws1KlJTU1FTk6OVBMTEwO1Wg13d3epZv/+/TpT6mNiYqDVauHg4FCbXSciIiI9VKtTY+bm5nBxcdFZZmZmhhYtWkjLg4KCMG/ePHTq1AmdOnXCvHnzYGpqCn9/fwCARqPBmDFjEBISghYtWqB58+YIDQ2Fq6urNPjayckJAwcOxNixY7FmzRoAwLhx4zBkyBA4OjoCAAYMGABnZ2cEBARg8eLFuHLlCkJDQzF27FhpdLi/vz/mzJmDwMBAfPDBB/jrr78wb948zJw5876n6oiIiEj/1SoIPYgpU6bgxo0bmDBhAvLz8+Hp6YmYmBiYm5tLNcuWLYOhoSFGjhyJGzduwMfHBxERETAwMJBqtmzZgsmTJ0uzy4YNG4ZVq1ZJ6w0MDLBjxw5MmDABvXv3homJCfz9/bFkyRKpRqPRIDY2FhMnToSHhwesrKwQHByM4ODgut5tIiIiaoQe6jpCSvKg1yG4F15HiIiI6NGr1+sIEREREekDBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUqxaBaHVq1fDzc0NFhYWsLCwgJeXF3755RdpvRACs2fPhlarhYmJCfr06YM///xT5z1KSkowadIkWFtbw8zMDMOGDcOFCxd0avLz8xEQEACNRgONRoOAgAAUFBTo1GRmZmLo0KEwMzODtbU1Jk+ejNLSUp2a48ePw9vbGyYmJmjdujXCwsIghKjNLhMREZEeq1UQatOmDRYsWIDDhw/j8OHD6NevH5577jkp7CxatAjh4eFYtWoVEhMTYWdnB19fX1y9elV6j6CgIERFRSEyMhJxcXG4du0ahgwZgvLycqnG398fKSkpiI6ORnR0NFJSUhAQECCtLy8vx+DBg1FcXIy4uDhERkZi+/btCAkJkWqKiorg6+sLrVaLxMRErFy5EkuWLEF4ePhDf1hERESkX1TiXx4iad68ORYvXow33ngDWq0WQUFBmDp1KoDbR39sbW2xcOFCjB8/HoWFhWjZsiU2bdqEl156CQCQnZ0Ne3t77Ny5E35+fjh58iScnZ0RHx8PT09PAEB8fDy8vLxw6tQpODo64pdffsGQIUOQlZUFrVYLAIiMjERgYCDy8vJgYWGB1atXY/r06bh48SLUajUAYMGCBVi5ciUuXLgAlUr1QPtXVFQEjUaDwsJCWFhY1PrzcZi2o9avqSsZCwbLtm0iIiI5Pej390OPESovL0dkZCSKi4vh5eWF9PR05ObmYsCAAVKNWq2Gt7c3Dhw4AABISkpCWVmZTo1Wq4WLi4tUc/DgQWg0GikEAUDPnj2h0Wh0alxcXKQQBAB+fn4oKSlBUlKSVOPt7S2FoMqa7OxsZGRk3HO/SkpKUFRUpPMgIiIi/VTrIHT8+HE0a9YMarUab731FqKiouDs7Izc3FwAgK2trU69ra2ttC43NxfGxsawsrKqscbGxqbKdm1sbHRq7t6OlZUVjI2Na6ypfF5ZU5358+dLY5M0Gg3s7e1r/kCIiIio0ap1EHJ0dERKSgri4+Px9ttvY9SoUThx4oS0/u5TTkKI+56Gurumuvq6qKk8C1hTP9OnT0dhYaH0yMrKqrF3IiIiarxqHYSMjY3RsWNHeHh4YP78+ejWrRs+/fRT2NnZAah6tCUvL086EmNnZ4fS0lLk5+fXWHPx4sUq27106ZJOzd3byc/PR1lZWY01eXl5AKoetbqTWq2WZsVVPoiIiEg//evrCAkhUFJSgvbt28POzg6xsbHSutLSUuzbtw+9evUCALi7u8PIyEinJicnB6mpqVKNl5cXCgsLcejQIakmISEBhYWFOjWpqanIycmRamJiYqBWq+Hu7i7V7N+/X2dKfUxMDLRaLRwcHP7tbhMREZEeqFUQ+uCDD/D7778jIyMDx48fx4cffoi9e/fi1VdfhUqlQlBQEObNm4eoqCikpqYiMDAQpqam8Pf3BwBoNBqMGTMGISEh2L17N5KTk/Haa6/B1dUV/fv3BwA4OTlh4MCBGDt2LOLj4xEfH4+xY8diyJAhcHR0BAAMGDAAzs7OCAgIQHJyMnbv3o3Q0FCMHTtWOoLj7+8PtVqNwMBApKamIioqCvPmzUNwcPADzxgjIiIi/WZYm+KLFy8iICAAOTk50Gg0cHNzQ3R0NHx9fQEAU6ZMwY0bNzBhwgTk5+fD09MTMTExMDc3l95j2bJlMDQ0xMiRI3Hjxg34+PggIiICBgYGUs2WLVswefJkaXbZsGHDsGrVKmm9gYEBduzYgQkTJqB3794wMTGBv78/lixZItVoNBrExsZi4sSJ8PDwgJWVFYKDgxEcHPxwnxQRERHpnX99HSF9x+sIERERNT71fh0hIiIiosaOQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBSLQYiIiIgUi0GIiIiIFItBiIiIiBTLUO4GSD85TNsh27YzFgyWbdtERNS48IgQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKZZhbYrnz5+P77//HqdOnYKJiQl69eqFhQsXwtHRUaoRQmDOnDlYu3Yt8vPz4enpic8++wxdu3aVakpKShAaGoqtW7fixo0b8PHxweeff442bdpINfn5+Zg8eTJ+/PFHAMCwYcOwcuVKWFpaSjWZmZmYOHEi9uzZAxMTE/j7+2PJkiUwNjaWao4fP4533nkHhw4dQvPmzTF+/HjMmDEDKpWq1h8W0f04TNsh27YzFgyWbdtERI1VrY4I7du3DxMnTkR8fDxiY2Nx69YtDBgwAMXFxVLNokWLEB4ejlWrViExMRF2dnbw9fXF1atXpZqgoCBERUUhMjIScXFxuHbtGoYMGYLy8nKpxt/fHykpKYiOjkZ0dDRSUlIQEBAgrS8vL8fgwYNRXFyMuLg4REZGYvv27QgJCZFqioqK4OvrC61Wi8TERKxcuRJLlixBeHj4Q31YREREpF9qdUQoOjpa5/n69ethY2ODpKQkPPPMMxBCYPny5fjwww8xYsQIAMCGDRtga2uLb775BuPHj0dhYSHWrVuHTZs2oX///gCAzZs3w97eHr/++iv8/Pxw8uRJREdHIz4+Hp6engCAL7/8El5eXkhLS4OjoyNiYmJw4sQJZGVlQavVAgCWLl2KwMBAzJ07FxYWFtiyZQtu3ryJiIgIqNVquLi44PTp0wgPD0dwcDCPChERESncvxojVFhYCABo3rw5ACA9PR25ubkYMGCAVKNWq+Ht7Y0DBw4AAJKSklBWVqZTo9Vq4eLiItUcPHgQGo1GCkEA0LNnT2g0Gp0aFxcXKQQBgJ+fH0pKSpCUlCTVeHt7Q61W69RkZ2cjIyOj2n0qKSlBUVGRzoOIiIj000MHISEEgoOD8dRTT8HFxQUAkJubCwCwtbXVqbW1tZXW5ebmwtjYGFZWVjXW2NjYVNmmjY2NTs3d27GysoKxsXGNNZXPK2vuNn/+fGg0Gulhb29/n0+CiIiIGquHDkLvvPMOjh07hq1bt1ZZd/cpJyHEfU9D3V1TXX1d1Agh7vlaAJg+fToKCwulR1ZWVo19ExERUeP1UEFo0qRJ+PHHH/Hbb7/pzPSys7MDUPVoS15ennQkxs7ODqWlpcjPz6+x5uLFi1W2e+nSJZ2au7eTn5+PsrKyGmvy8vIAVD1qVUmtVsPCwkLnQURERPqpVkFICIF33nkH33//Pfbs2YP27dvrrG/fvj3s7OwQGxsrLSstLcW+ffvQq1cvAIC7uzuMjIx0anJycpCamirVeHl5obCwEIcOHZJqEhISUFhYqFOTmpqKnJwcqSYmJgZqtRru7u5Szf79+1FaWqpTo9Vq4eDgUJtdJyIiIj1Uq1ljEydOxDfffIP//e9/MDc3l462aDQamJiYQKVSISgoCPPmzUOnTp3QqVMnzJs3D6ampvD395dqx4wZg5CQELRo0QLNmzdHaGgoXF1dpVlkTk5OGDhwIMaOHYs1a9YAAMaNG4chQ4ZI1ywaMGAAnJ2dERAQgMWLF+PKlSsIDQ3F2LFjpaM4/v7+mDNnDgIDA/HBBx/gr7/+wrx58zBz5kzOGCOqQ7x+EhE1VrUKQqtXrwYA9OnTR2f5+vXrERgYCACYMmUKbty4gQkTJkgXVIyJiYG5ublUv2zZMhgaGmLkyJHSBRUjIiJgYGAg1WzZsgWTJ0+WZpcNGzYMq1atktYbGBhgx44dmDBhAnr37q1zQcVKGo0GsbGxmDhxIjw8PGBlZYXg4GAEBwfXZreJiIhIT6lE5ehhqlZRURE0Gg0KCwsfaryQUv9S5n4/etxvIqL/86Df37zXGBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESmWodwNEBE1Vg7Tdsi27YwFg2XbNpE+4REhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUqxaB6H9+/dj6NCh0Gq1UKlU+OGHH3TWCyEwe/ZsaLVamJiYoE+fPvjzzz91akpKSjBp0iRYW1vDzMwMw4YNw4ULF3Rq8vPzERAQAI1GA41Gg4CAABQUFOjUZGZmYujQoTAzM4O1tTUmT56M0tJSnZrjx4/D29sbJiYmaN26NcLCwiCEqO1uExERkR6qdRAqLi5Gt27dsGrVqmrXL1q0COHh4Vi1ahUSExNhZ2cHX19fXL16VaoJCgpCVFQUIiMjERcXh2vXrmHIkCEoLy+Xavz9/ZGSkoLo6GhER0cjJSUFAQEB0vry8nIMHjwYxcXFiIuLQ2RkJLZv346QkBCppqioCL6+vtBqtUhMTMTKlSuxZMkShIeH13a3iYiISA8Z1vYFgwYNwqBBg6pdJ4TA8uXL8eGHH2LEiBEAgA0bNsDW1hbffPMNxo8fj8LCQqxbtw6bNm1C//79AQCbN2+Gvb09fv31V/j5+eHkyZOIjo5GfHw8PD09AQBffvklvLy8kJaWBkdHR8TExODEiRPIysqCVqsFACxduhSBgYGYO3cuLCwssGXLFty8eRMRERFQq9VwcXHB6dOnER4ejuDgYKhUqof60IiIiEg/1OkYofT0dOTm5mLAgAHSMrVaDW9vbxw4cAAAkJSUhLKyMp0arVYLFxcXqebgwYPQaDRSCAKAnj17QqPR6NS4uLhIIQgA/Pz8UFJSgqSkJKnG29sbarVapyY7OxsZGRnV7kNJSQmKiop0HkRERKSf6jQI5ebmAgBsbW11ltva2krrcnNzYWxsDCsrqxprbGxsqry/jY2NTs3d27GysoKxsXGNNZXPK2vuNn/+fGlckkajgb29/f13nIiIiBqlepk1dvcpJyHEfU9D3V1TXX1d1FQOlL5XP9OnT0dhYaH0yMrKqrFvIiIiarzqNAjZ2dkBqHq0JS8vTzoSY2dnh9LSUuTn59dYc/HixSrvf+nSJZ2au7eTn5+PsrKyGmvy8vIAVD1qVUmtVsPCwkLnQURERPqpToNQ+/btYWdnh9jYWGlZaWkp9u3bh169egEA3N3dYWRkpFOTk5OD1NRUqcbLywuFhYU4dOiQVJOQkIDCwkKdmtTUVOTk5Eg1MTExUKvVcHd3l2r279+vM6U+JiYGWq0WDg4OdbnrRERE1AjVOghdu3YNKSkpSElJAXB7gHRKSgoyMzOhUqkQFBSEefPmISoqCqmpqQgMDISpqSn8/f0BABqNBmPGjEFISAh2796N5ORkvPbaa3B1dZVmkTk5OWHgwIEYO3Ys4uPjER8fj7Fjx2LIkCFwdHQEAAwYMADOzs4ICAhAcnIydu/ejdDQUIwdO1Y6iuPv7w+1Wo3AwECkpqYiKioK8+bN44wxIiIiAvAQ0+cPHz6Mvn37Ss+Dg4MBAKNGjUJERASmTJmCGzduYMKECcjPz4enpydiYmJgbm4uvWbZsmUwNDTEyJEjcePGDfj4+CAiIgIGBgZSzZYtWzB58mRpdtmwYcN0rl1kYGCAHTt2YMKECejduzdMTEzg7++PJUuWSDUajQaxsbGYOHEiPDw8YGVlheDgYKlnIiIiUrZaB6E+ffrUeGVmlUqF2bNnY/bs2fesadq0KVauXImVK1fes6Z58+bYvHlzjb20bdsWP//8c401rq6u2L9/f401REREpEy81xgREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESmWodwNEBFR4+IwbYds285YMFi2bZN+4hEhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLAYhIiIiUiwGISIiIlIsBiEiIiJSLF5ZmoiI6AHwitr6iUeEiIiISLEYhIiIiEixGISIiIhIsRiEiIiISLEYhIiIiEixOGuMiIiI7knfZ8vxiBAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpliKC0Oeff4727dujadOmcHd3x++//y53S0RERNQA6H0Q+vbbbxEUFIQPP/wQycnJePrppzFo0CBkZmbK3RoRERHJTO+DUHh4OMaMGYM333wTTk5OWL58Oezt7bF69Wq5WyMiIiKZGcrdQH0qLS1FUlISpk2bprN8wIABOHDgQLWvKSkpQUlJifS8sLAQAFBUVPRQPVSUXH+o19WFh+25LnC/Hz3u96PH/X70uN+PXmPd78rXCiFqLhR67O+//xYAxB9//KGzfO7cuaJz587VvmbWrFkCAB988MEHH3zwoQePrKysGrOCXh8RqqRSqXSeCyGqLKs0ffp0BAcHS88rKipw5coVtGjR4p6vqS9FRUWwt7dHVlYWLCwsHum25cT95n4rAfeb+60Ecu63EAJXr16FVqutsU6vg5C1tTUMDAyQm5urszwvLw+2trbVvkatVkOtVusss7S0rK8WH4iFhYWi/uFU4n4rC/dbWbjfyiLXfms0mvvW6PVgaWNjY7i7uyM2NlZneWxsLHr16iVTV0RERNRQ6PURIQAIDg5GQEAAPDw84OXlhbVr1yIzMxNvvfWW3K0RERGRzPQ+CL300ku4fPkywsLCkJOTAxcXF+zcuRPt2rWTu7X7UqvVmDVrVpVTdfqO+839VgLuN/dbCRrDfquEuN+8MiIiIiL9pNdjhIiIiIhqwiBEREREisUgRERERIrFIERERESKxSDUgKSnp8vdAhERkaIwCDUgHTt2RN++fbF582bcvHlT7naonu3fvx+3bt2qsvzWrVvYv3+/DB0RUX1Ryu/0W7duYc6cOcjKypK7lQfGINSAHD16FI8//jhCQkJgZ2eH8ePH49ChQ3K39chUVFTg9OnTiIuLw/79+3Ue+qhv3764cuVKleWFhYXo27evDB09Ojdu3MD16/93R+vz589j+fLliImJkbGr+tevXz8UFBRUWV5UVIR+/fo9+obqWVlZGTp06IATJ07I3YosKioq8PHHH6N169Zo1qwZzp07BwCYMWMG1q1bJ3N39cPQ0BCLFy9GeXm53K08uDq5zTvVqbKyMvH999+LYcOGCSMjI+Hs7CyWLl0q8vLy5G6t3hw8eFC0b99eNGnSRKhUKp1HkyZN5G6vXqhUqmp/pmlpacLc3FyGjh4dX19fsXr1aiGEEPn5+cLW1la0adNGNG3aVHz++ecyd1d/VCqVuHjxYpXlFy9eFIaGhjJ0VP+0Wq04ceKE3G3IYs6cOaJDhw5i8+bNwsTERJw9e1YIIcS3334revbsKXN39ee5554T69evl7uNB8YLKjZgJSUl+PzzzzF9+nSUlpbCyMgIL730EhYuXIhWrVrJ3V6d6t69Ozp37ow5c+agVatWUKlUOusf5MZ5jcWIESMAAP/73/8wcOBAnSuulpeX49ixY3B0dER0dLRcLdY7a2tr7Nu3D127dsVXX32FlStXIjk5Gdu3b8fMmTNx8uRJuVusU8eOHQNw+//zPXv2oHnz5tK68vJyREdHY82aNcjIyJCpw/qzYMECnDp1Cl999RUMDfX+ZgY6OnbsiDVr1sDHxwfm5uY4evQoOnTogFOnTsHLywv5+flyt1gv1qxZg9mzZ+PVV1+Fu7s7zMzMdNYPGzZMps6qp6z/KxuJw4cP4+uvv0ZkZCTMzMwQGhqKMWPGIDs7GzNnzsRzzz2nd6fM/vrrL2zbtg0dO3aUu5V6VxnqhBAwNzeHiYmJtM7Y2Bg9e/bE2LFj5Wrvkbh+/TrMzc0BADExMRgxYgSaNGmCnj174vz58zJ3V/e6d+8OlUoFlUpV7SkwExMTrFy5UobO6l9CQgJ2796NmJgYuLq6VvlS/P7772XqrP79/fff1f5Oq6ioQFlZmQwdPRpvv/02ACA8PLzKOpVK1eBOmzEINSDh4eFYv3490tLS8Oyzz2Ljxo149tln0aTJ7aFc7du3x5o1a9ClSxeZO617np6eOHPmjCKC0Pr16wEADg4OCA0NrfLFoAQdO3bEDz/8gOHDh2PXrl147733AAB5eXmwsLCQubu6l56eDiEEOnTogEOHDqFly5bSOmNjY9jY2MDAwEDGDuuPpaUlXnjhBbnbkEXXrl3x+++/V7m35X//+188/vjjMnVV/yoqKuRuoVYYhBqQ1atX44033sDo0aNhZ2dXbU3btm31cpDdpEmTEBISgtzcXLi6usLIyEhnvZubm0yd1Z8pU6bgzjPT58+fR1RUFJydnTFgwAAZO6t/M2fOhL+/P9577z34+PjAy8sLwO2jQ/r4BVH5RdjYviDqQmXwV6JZs2YhICAAf//9NyoqKvD9998jLS0NGzduxM8//yx3e4/EzZs30bRpU7nbqJmsI5SI/r+7B0hXDpLW58HSdw8YtrGxUcSA4Uo5OTniyJEjory8XFqWkJAgTp48KWNX9SsiIkL8/PPP0vP3339faDQa4eXlJTIyMmTsjOpLdHS0eOaZZ4SZmZkwMTERvXv3Frt27ZK7rXp169YtERYWJrRarTAwMJAGiX/00Ufiq6++krm7qjhYuoEpKCjAunXrcPLkSahUKjg5OWHMmDF6NVi4OvcbF3L3oWV9oLQBwzUpKirCnj174OjoCCcnJ7nbqTeOjo5YvXo1+vXrh4MHD8LHxwfLly/Hzz//DENDQ70dL7Nt2zZ89913yMzMRGlpqc66I0eOyNQV1ZewsDBs2LABYWFhGDt2LFJTU9GhQwd89913WLZsGQ4ePCh3izp4HaEG5PDhw3jsscewbNkyXLlyBf/88w+WLVuGxx57TO9/WbRr167Ghz5S2oDhO40cORKrVq0CcPuaQh4eHhg5ciTc3Nywfft2mburP1lZWdI4uB9++AEvvvgixo0bh/nz5+P333+Xubv6sWLFCowePRo2NjZITk5Gjx490KJFC5w7dw6DBg2Su716lZWVhQsXLkjPDx06hKCgIKxdu1bGrurfxo0bsXbtWrz66qs6Y9/c3Nxw6tQpGTurHoNQA/Lee+9h2LBhyMjIwPfff4+oqCikp6djyJAhCAoKkru9enf27FlMmjQJ/fv3h6+vLyZPnoyzZ8/K3Va9qRwwnJWVhV27dknjgvR1wPCd9u/fj6effhoAEBUVBSEECgoKsGLFCnzyyScyd1d/mjVrhsuXLwO4HX779+8PAGjatClu3LghZ2v15vPPP8fatWuxatUqGBsbY8qUKYiNjcXkyZNRWFgod3v1yt/fH7/99hsAIDc3F/3798ehQ4fwwQcfICwsTObu6k9jmy3HINSAHD58GFOnTtW51oahoSGmTJmCw4cPy9hZ/du1axecnZ1x6NAhuLm5wcXFBQkJCejatStiY2Plbq9ezJw5E6GhoXBwcICnp6feDxi+U2FhoXQtnejoaLzwwgswNTXF4MGD8ddff8ncXf3x9fXFm2++iTfffBOnT5/G4MGDAQB//vknHBwc5G2unmRmZqJXr14Abl8m4OrVqwCAgIAAbN26Vc7W6l1qaip69OgBAPjuu+/g6uqKAwcO4JtvvkFERIS8zdWjytlyd2uos+U4a6wBsbCwQGZmZpXp8VlZWdIpFH01bdo0vPfee1iwYEGV5VOnToWvr69MndWfF198EU899RRycnLQrVs3abmPjw+GDx8uY2f1z97eHgcPHkTz5s0RHR2NyMhIAEB+fn7Dn2HyL3z22Wf46KOPkJWVhe3bt6NFixYAgKSkJLzyyisyd1c/7OzscPnyZek0d3x8PLp16yZdUkCflZWVSRdM/fXXX6ULCXbp0gU5OTlytlavGt1sOXnHatOdJk2aJNq0aSMiIyNFZmamyMrKElu3bhVt2rQR7777rtzt1Su1Wi1Onz5dZXlaWppQq9UydET16bPPPhOGhobC0tJSuLm5STPHVqxYIfr06SNzd1SXxowZI2bPni2EEGL16tXCxMRE9O/fX1haWoo33nhD5u7qV48ePcTUqVPF/v37RdOmTUVKSooQ4vYthVq3bi1zd/WrMc2W46yxBqS0tBTvv/8+vvjiC9y6dQtCCBgbG+Ptt9/GggULdG7FoG/s7e0RHh6O//znPzrLv/vuO4SGhiIzM1OmzupP3759q9xK5E579ux5hN08eocPH0ZWVhZ8fX3RrFkzAMCOHTtgaWmJ3r17y9xd/bjfDYSfeeaZR9TJo1NRUYGKigrplP93332HuLg4dOzYEW+99RaMjY1l7rD+7N27F8OHD0dRURFGjRqFr7/+GgDwwQcf4NSpU3o7S7CxYRBqgK5fv46zZ89CCIGOHTvC1NRU7pbqXVhYGJYtW4Zp06ahV69eUKlUiIuLw8KFCxESEoKPPvpI7hbrXOXVlCuVlZUhJSUFqampGDVqFD799FOZOnt0SktLkZ6ejscee0wR96GqvEr8ne4Mww3t1gP075WXl6OoqAhWVlbSsoyMDJiamsLGxkbGzqgSg5DMRowYgYiICFhYWEg347yXZs2aoWvXrnjrrbf07rpCQggsX74cS5cuRXZ2NgBAq9Xi/fffx+TJk2s8cqJvZs+ejWvXrmHJkiVyt1Jvrl+/jkmTJmHDhg0AgNOnT6NDhw6YPHkytFotpk2bJnOH9ePuWVJlZWVITk7GjBkzMHfuXPj4+MjUWf36/fffsWbNGpw9exbbtm1D69atsWnTJrRv3x5PPfWU3O1RHbOysqr2d7ZKpULTpk3RsWNHBAYGYvTo0TJ0V5X+/wnWwGk0Gul/mPuFm5KSEnzxxRf4448/8OOPPz6K9h4ZlUqF9957D++99540q0TfB4jfy2uvvYYePXrodRCaPn06jh49ir1792LgwIHS8v79+2PWrFl6G4Sq+zfu6+sLtVqN9957D0lJSTJ0Vb+2b9+OgIAAvPrqq0hOTkZJSQkA4OrVq5g3bx527twpc4d164knnsDu3bthZWWFxx9/vMY/4vT1+nAzZ87E3LlzMWjQIPTo0QNCCCQmJiI6OhoTJ05Eeno63n77bdy6datB3GCaQUhmd96H50HuyXPixAk8+eST9dmS7JQagCodPHhQr2dOAbcvJvjtt9+iZ8+eOl8Uzs7Oen3tqHtp2bIl0tLS5G6jXnzyySf44osv8Prrr0uzAwGgV69eenktneeee04az/n888/L24xM4uLi8Mknn+Ctt97SWb5mzRrExMRg+/btcHNzw4oVKxiEqPYcHR1x4MABuduoE0r/y+nuU6FCCOTk5ODw4cOYMWOGTF09GpcuXap2fERxcbFenwY9duyYzvPKn/mCBQt0LqGgT9LS0qodBG5hYYGCgoJH31A9mzVrFoDbY4P69OkDNzc3nfFBSrBr1y4sXLiwynIfHx+EhIQAAJ599tkGc+SXQaiRMTAw0JtfmHf+5fTcc8/p9Rdgde4+TdKkSRM4OjoiLCxM7+8+/+STT2LHjh2YNGkSgP8bMPzll19KF5bUR927d4dKpapy/ZyePXtKM4r0TatWrXDmzJkqF4yMi4tDhw4d5GnqETAwMICfnx9OnjypuCDUvHlz/PTTT1UmhPz000/ShVSLi4sbzNF/BiGSTeVfTsDtAcJKUl5ejsDAQLi6ukq/GJRk/vz5GDhwIE6cOIFbt27h008/xZ9//omDBw9i3759crdXb9LT03WeN2nSBC1bttTrU6Hjx4/Hu+++i6+//hoqlQrZ2dk4ePAgQkNDMXPmTLnbq1eurq44d+4c2rdvL3crj9SMGTPw9ttv47fffkOPHj2gUqlw6NAh7Ny5E1988QUAIDY2Ft7e3jJ3ehtnjVGD0KFDByQmJkpX2q1UUFCAJ554AufOnZOps/rTtGlTnDx5UnG/JCsdP34cS5YsQVJSEioqKvDEE09g6tSpcHV1lbs1+peOHTsGFxcX6XIBH374IZYtW4abN28CANRqNUJDQ/Hxxx/L2Wa9i4mJwdSpU/Hxxx/D3d0dZmZmOuv1+Z6Cf/zxB1atWoW0tDQIIdClSxdMmjRJut1KQ8IgRA1CkyZNkJubW2XcyMWLF2Fvb4/S0lKZOqs/Tz75JBYsWKC3U6apqhUrVlS7/M5pxc8884zOHbsbIwMDA+Tk5MDGxkb6I6cy+FdUVMDZ2Vm6iKY+u/O6UXee+hdCQKVS8bpRDQRPjZGs7rwMwK5du3TGzZSXl2P37t16e8Rk7ty50l/FSvtrEbh9xeEzZ84gLy8PFRUVOuv08QrLALBs2TJcunQJ169fh5WVFYQQKCgogKmpKZo1a4a8vDx06NABv/32G+zt7eVu96FZWloiPT0dNjY2yMjIQEVFBczMzODh4SF3a49U5Z3nlagx/fvmESGSVeVfTNUNIDUyMoKDgwOWLl2KIUOGyNFevVLyX4vx8fHw9/fH+fPnq/zc9Xnft27dirVr1+Krr77CY489BgA4c+YMxo8fj3HjxqF37954+eWXYWdnh23btsnc7cMbN24cNm7ciFatWiEzMxNt2rS551EufTztrXSN7d83gxA1CO3bt0diYiKsra3lbuWR2bBhA+zt7at8QVRUVCAzMxOjRo2SqbP61717d3Tu3Blz5sxBq1atqswY1Lcrp1d67LHHsH37dnTv3l1neXJyMl544QWcO3cOBw4cwAsvvNDo704eHR2NM2fOYPLkyQgLC7vnDKF33333EXf2aOXn52PdunU4efIkVCoVnJycMHr0aL2eJNHY/n0zCBHJ5M5xFHe6fPkybGxsGtxfTXXJzMwMR48eRceOHeVu5ZEyNTXF/v37q5wiSkxMhLe3N65fv46MjAy4uLjg2rVrMnVZt0aPHo0VK1Y0mKnSj9K+ffswbNgwaDQa6WeelJSEgoIC/Pjjjw1m1lRda2z/vjlGiBqM4uJi7Nu3D5mZmVUGR0+ePFmmrupP5Smwu127dk2vp1MDgKenJ86cOdNoflHWlb59+2L8+PH46quv8PjjjwO4fTTo7bffRr9+/QDcnk2nT+PiHuSK+fpq4sSJeOmll7B69WrpyG95eTkmTJiAiRMnIjU1VeYO60dj+/fNIEQNQnJyMp599llcv34dxcXFaN68Of755x/pDs36FISCg4MB3D5XPmPGDJiamkrrysvLkZCQUOXUib6ZNGkSQkJCkJubC1dXVxgZGemsd3Nzk6mz+rVu3ToEBATA3d1d2udbt27Bx8cH69atA3D75spLly6Vs02qI2fPnsX27dt1Tn8bGBggODgYGzdulLGz+tXY/n3z1Bg1CH369EHnzp2xevVqWFpa4ujRozAyMsJrr72Gd999t8rtKBqzvn37Arh92NzLywvGxsbSOmNjYzg4OCA0NBSdOnWSq8V6d+dA8UqVA+Yb4mDKupaWlqZzfRVHR0e5W6J60Lt3b7z//vtV7jn2ww8/YOHChTh48KA8jdWz6v59V2qI/74ZhKhBsLS0REJCAhwdHWFpaYmDBw/CyckJCQkJGDVqFE6dOiV3i3Vu9OjR+PTTT/V+mnx1zp8/X+P6du3aPaJO5FVeXo7jx4+jXbt2irsNgxJ8++23mDJlCiZNmoSePXsCuD2j6rPPPsOCBQvg5OQk1Ta0oyT/RmP7980gRA1Cy5Yt8ccff6Bz585wdHTEihUr4Ofnh1OnTuGJJ57A9evX5W6R6F8LCgqCq6srxowZg/Lycnh7e+PAgQMwNTXFzz//jD59+sjdItWhmo6MAPp/FPTEiRNVxnyqVCoMHTpUxq6q4hghahAef/xxHD58GJ07d0bfvn0xc+ZM/PPPP9i0aRNvuaAn7rx45v0MGzasHjuRz7Zt2/Daa68BuH0DynPnzuHUqVPYuHEjPvzwQ/zxxx8yd0h16e57yynFuXPnMHz4cBw/flznGnGVk0MaWujjESFqEA4fPoyrV6+ib9++uHTpEkaNGoW4uDh07NgR69evR7du3eRukf6lu/86vvsimnfOoGtovyjrStOmTXHmzBm0adMG48aNg6mpKZYvX4709HR069YNRUVFcrdIdaSsrAzjxo3DjBkz0KFDB7nbeaSGDh0KAwMDfPnll+jQoQMSEhJw5coVhISEYMmSJXj66aflblFHzcftiB4RDw8PaRBxy5YtsXPnThQVFeHIkSMMQXqioqJCesTExKB79+745ZdfUFBQgMLCQuzcuRNPPPEEoqOj5W613tja2uLEiRMoLy9HdHQ0+vfvDwC4fv16o7+/GOkyMjJCVFSU3G3I4uDBgwgLC0PLli3RpEkTGBgY4KmnnsL8+fMb5AxgnhqjBiUvLw9paWlQqVRwdHREy5Yt5W6J6kFQUBC++OILPPXUU9IyPz8/mJqaYty4cTh58qSM3dWf0aNHY+TIkdLVdn19fQEACQkJ6NKli8zdUV0bPnw4fvjhB+mSGUpRXl4u3VTX2toa2dnZcHR0RLt27ZCWliZzd1UxCFGDUFRUhIkTJyIyMlI6LWJgYICXXnoJn332WYO7JDv9O2fPnq32Z6rRaJCRkfHoG3pEZs+eDRcXF2RlZeE///kP1Go1gNv/r0+bNk3m7qiudezYER9//DEOHDhQ7Y2VG+LRkbrg4uKCY8eOoUOHDvD09MSiRYtgbGyMtWvXNsjThBwjRA3CyJEjkZKSgpUrV8LLywsqlQoHDhzAu+++Czc3N3z33Xdyt0h16JlnnoGRkRE2b96MVq1aAQByc3MREBCA0tJS7Nu3T+YOif69mq4QrlKp9PaGs7t27UJxcTFGjBiBc+fOYciQITh16hRatGiBb7/9VrqKekPBIEQNgpmZGXbt2qVzqgQAfv/9dwwcOBDFxcUydUb14cyZMxg+fDjS0tLQtm1bAEBmZiY6d+6MH374odFcmv9BrFixAuPGjUPTpk2xYsWKGmv19QgB0ZUrV2BlZVXtbYXkxiBEDULbtm2xY8eOKlPljx07hmeffRYXLlyQqTOqL0IIxMbG4tSpUxBCwNnZGf3792+Qvyj/jfbt2+Pw4cNo0aKFYo8QEDVkDELUIKxduxb//e9/sXHjRp1TJaNGjcKIESMwfvx4mTskIqqdN954o8b1X3/99SPqhGrCIEQNwuOPP44zZ86gpKRE51SJWq2ucs+tI0eOyNEi1bHi4mLs27evypVnAf06RfSgM4ZUKhVvtqpnhg8frvO8rKwMqampKCgoQL9+/fD999/L1BndibPGqEG4+6aEpN+Sk5Px7LPP4vr16yguLkbz5s3xzz//wNTUFDY2NnoVhJKTk3WeJyUloby8XLrR6unTp2FgYAB3d3c52qN6VN11hCoqKjBhwoQGOXtKqXhEiIgeuT59+qBz585YvXo1LC0tcfToURgZGeG1117Du+++ixEjRsjdYr0IDw/H3r17sWHDBukmq/n5+Rg9ejSefvpphISEyNwhPQppaWno06cPcnJy5G6FwCBERDKwtLREQkICHB0dYWlpiYMHD8LJyQkJCQkYNWoUTp06JXeL9aJ169aIiYlB165ddZanpqZiwIAByM7OlqkzepR27tyJUaNG4dKlS3K3QuCpMZJR8+bNcfr0aVhbW993WuWVK1ceYWdU34yMjKSft62tLTIzM+Hk5ASNRoPMzEyZu6s/RUVFuHjxYpUglJeXh6tXr8rUFdWXu8eHCSGQk5ODHTt2YNSoUTJ1RXdjECLZLFu2DObm5tJ/69u0abq3xx9/HIcPH0bnzp3Rt29fzJw5E//88w82bdpU5RIK+mT48OEYPXo0li5dip49ewIA4uPj8f777+vt6UAlu3t8WJMmTdCyZUssXbr0vjPK6NHhqTEieuQOHz6Mq1evom/fvrh06RJGjRqFuLg4dOrUCevWrUP37t3lbrFeXL9+HaGhofj6669RVlYGADA0NMSYMWOwePHiKrdgoMbt+vXrEEJIP9eMjAz88MMPcHJygp+fn8zdUSUGIWoQdu7cCQMDgyq/HGJiYlBeXo5BgwbJ1BnVhxs3bkAIAVNTUwC3vyCioqLg7OysiC+I4uJinD17FkIIdOzYkQFITw0YMAAjRozAW2+9hYKCAnTp0gVGRkb4559/EB4ejrffflvuFglAE7kbIAKAadOmSTdbvVNFRQVvRqmHnnvuOWzcuBEAUFBQgJ49eyI8PBzPP/88Vq9eLXN39c/MzAxubm7o1q0bQ5AeO3LkCJ5++mkAwLZt22Bra4vz589j48aN973dCj06DELUIPz1119wdnausrxLly44c+aMDB1RfeIXBCnB9evXpXGQMTExGDFiBJo0aYKePXvi/PnzMndHlRiEqEHQaDTV3mfpzJkz/ItZD/ELgpSgY8eO+OGHH5CVlYVdu3ZhwIABAG7PErSwsJC5O6rEIEQNwrBhwxAUFISzZ89Ky86cOYOQkBAMGzZMxs6oPvALgpRg5syZCA0NhYODAzw9PeHl5QXgdvh//PHHZe6OKnGwNDUIhYWFGDhwIA4fPow2bdoAAC5cuICnn34a33//PSwtLeVtkOrUtm3b4O/vj/Lycvj4+CAmJgYAMH/+fOzfvx+//PKLzB0S1Y3c3Fzk5OSgW7duaNLk9rGHQ4cOwcLCAl26dJG5OwIYhKgBEUIgNjYWR48ehYmJCdzc3PDMM8/I3RbVE35BEFFDwCBEREREisUrS5NsVqxYgXHjxqFp06b3nSmkT3cjJyKihoNHhEg27du3x+HDh9GiRQu0b9/+nnUqlaraGWVERET/FoMQERERKRanz1ODEBYWhuvXr1dZfuPGDYSFhcnQERERKQGPCFGDYGBggJycHNjY2Ogsv3z5MmxsbKq9/QYREdG/xSNC1CAIIaBSqaosP3r0KJo3by5DR0REpAScNUaysrKygkqlgkqlQufOnXXCUHl5Oa5du4a33npLxg6JiEif8dQYyWrDhg0QQuCNN97A8uXLodFopHXGxsZwcHCQLktPRERU1xiEqEHYt28fevXqBSMjI7lbISIiBWEQogajoqICZ86cQV5eHioqKnTW8VYbRERUHzhGiBqE+Ph4+Pv74/z587g7m6tUKs4aIyKiesEjQtQgdO/eHZ07d8acOXPQqlWrKjPI7hw7REREVFcYhKhBMDMzw9GjR9GxY0e5WyEiIgXhdYSoQfD09MSZM2fkboOIiBSGY4SoQZg0aRJCQkKQm5sLV1fXKrPH3NzcZOqMiIj0GU+NUYPQpMm9D05ysDQREdUXHhGiBiE9PV3uFoiISIEYhKhBaNeuHQDgxIkTyMzMRGlpqbROpVJJ64mIiOoSgxA1COfOncPw4cNx/PhxqFQq6VpCldPoeWqMiIjqA2eNUYPw7rvvon379rh48SJMTU2RmpqK/fv3w8PDA3v37pW7PSIi0lMcLE0NgrW1Nfbs2QM3NzdoNBocOnQIjo6O2LNnD0JCQpCcnCx3i0REpId4RIgahPLycjRr1gzA7VCUnZ0N4PbYobS0NDlbIyIiPcYxQtQguLi44NixY+jQoQM8PT2xaNEiGBsbY+3atejQoYPc7RERkZ7iqTFqEHbt2oXi4mKMGDEC586dw5AhQ3Dq1Cm0aNEC3377Lfr16yd3i0REpIcYhKjBunLlCqysrKrcgJWIiKiuMAgRERGRYnGwNBERESkWgxAREREpFoMQERERKRaDEBERESkWgxAREREpFoMQEemF2bNno3v37nK3QUSNDIMQEVE9KCsrk7sFInoADEJE1GBUVFRg4cKF6NixI9RqNdq2bYu5c+cCAKZOnYrOnTvD1NQUHTp0wIwZM6SwERERgTlz5uDo0aNQqVRQqVSIiIgAABQWFmLcuHGwsbGBhYUF+vXrh6NHj+ps95NPPoGNjQ3Mzc3x5ptvYtq0aTpHlyoqKhAWFoY2bdpArVaje/fuiI6OltZnZGRApVLhu+++Q58+fdC0aVOsXbsWFhYW2LZtm862fvrpJ5iZmeHq1av18AkSUW0xCBFRgzF9+nQsXLgQM2bMwIkTJ/DNN9/A1tYWAGBubo6IiAicOHECn376Kb788kssW7YMAPDSSy8hJCQEXbt2RU5ODnJycvDSSy9BCIHBgwcjNzcXO3fuRFJSEp544gn4+PjgypUrAIAtW7Zg7ty5WLhwIZKSktC2bVusXr1ap69PP/0US5cuxZIlS3Ds2DH4+flh2LBh+Ouvv3Tqpk6dismTJ+PkyZMYPnw4Xn75Zaxfv16nZv369XjxxRdhbm5eXx8jEdWGICJqAIqKioRarRZffvnlA9UvWrRIuLu7S89nzZolunXrplOze/duYWFhIW7evKmz/LHHHhNr1qwRQgjh6ekpJk6cqLO+d+/eOu+l1WrF3LlzdWqefPJJMWHCBCGEEOnp6QKAWL58uU5NQkKCMDAwEH///bcQQohLly4JIyMjsXfv3gfaRyKqfzwiREQNwsmTJ1FSUgIfH59q12/btg1PPfUU7Ozs0KxZM8yYMQOZmZk1vmdSUhKuXbuGFi1aoFmzZtIjPT0dZ8+eBQCkpaWhR48eOq+783lRURGys7PRu3dvnZrevXvj5MmTOss8PDyqvE/Xrl2xceNGAMCmTZvQtm1bPPPMMzX2TUSPjqHcDRARAYCJick918XHx+Pll1/GnDlz4OfnB41Gg8jISCxdurTG96yoqECrVq2wd+/eKussLS2l/777xr6imlswVldz9zIzM7Mqr3vzzTexatUqTJs2DevXr8fo0aN5I2GiBoRHhIioQejUqRNMTEywe/fuKuv++OMPtGvXDh9++CE8PDzQqVMnnD9/XqfG2NgY5eXlOsueeOIJ5ObmwtDQEB07dtR5WFtbAwAcHR1x6NAhndcdPnxY+m8LCwtotVrExcXp1Bw4cABOTk733a/XXnsNmZmZWLFiBf7880+MGjXqvq8hokeHR4SIqEFo2rQppk6diilTpsDY2Bi9e/fGpUuX8Oeff6Jjx47IzMxEZGQknnzySezYsQNRUVE6r3dwcEB6ejpSUlLQpk0bmJubo3///vDy8sLzzz+PhQsXwtHREdnZ2di5cyeef/55eHh4YNKkSRg7diw8PDzQq1cvfPvttzh27Bg6dOggvff777+PWbNm4bHHHkP37t2xfv16pKSkYMuWLffdLysrK4wYMQLvv/8+BgwYgDZt2tT5Z0dE/4Lcg5SIiCqVl5eLTz75RLRr104YGRmJtm3binnz5gkhhHj//fdFixYtRLNmzcRLL70kli1bJjQajfTamzdvihdeeEFYWloKAGL9+vVCiNuDsCdNmiS0Wq0wMjIS9vb24tVXXxWZmZnSa8PCwoS1tbVo1qyZeOONN8TkyZNFz549dfqaM2eOaN26tTAyMhLdunUTv/zyi7S+crB0cnJytfu1e/duAUB89913dfdhEVGdUAlRzclwIiIF8/X1hZ2dHTZt2lQn77dlyxa8++67yM7OhrGxcZ28JxHVDZ4aIyJFu379Or744gv4+fnBwMAAW7duxa+//orY2Ng6ee/09HTMnz8f48ePZwgiaoA4WJqIFE2lUmHnzp14+umn4e7ujp9++gnbt29H//79//V7L1q0CN27d4etrS2mT59eB90SUV3jqTEiIiJSLB4RIiIiIsViECIiIiLFYhAiIiIixWIQIiIiIsViECIiIiLFYhAiIiIixWIQIiIiIsViECIiIiLF+n+ZeoDS9usrLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_train.groupby(['emotion']).count()['text'])\n",
    "df_train['emotion'].value_counts().plot(kind='bar')\n",
    "plt.title('Emotion class count')\n",
    "plt.xlabel(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a6209",
   "metadata": {},
   "source": [
    "Since Bert model is a more complex model, here I don't sample the data in order to get enough data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c005e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "anger           39867\n",
      "anticipation    39867\n",
      "disgust         39867\n",
      "fear            39867\n",
      "joy             39867\n",
      "sadness         39867\n",
      "surprise        39867\n",
      "trust           39867\n",
      "Name: text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# condition = df_train['emotion'] == 'anger'\n",
    "# df_select = df_train[condition].sample(39867)\n",
    "# condition = df_train['emotion'] == 'anticipation'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'disgust'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'fear'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'joy'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'sadness'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'surprise'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'trust'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "\n",
    "# df_train = df_select.reset_index(drop=True)\n",
    "# print(df_train.groupby(['emotion']).count()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae185e5",
   "metadata": {},
   "source": [
    "## 3. Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906f6bd",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcedde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f0de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train.text.values\n",
    "emotion = df_train.emotion.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b65d40",
   "metadata": {},
   "source": [
    "#### y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd18d4",
   "metadata": {},
   "source": [
    "Transform the label into one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d70c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with label into one hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f665fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = label_encode(label_encoder, emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56beb7bd",
   "metadata": {},
   "source": [
    "#### X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00bb52",
   "metadata": {},
   "source": [
    "Tokenize the text as the preprocessing. Here I use the pre-trained Bert Tokenizer `bert-base-uncased`. Bert tokenizer follows the following process: \n",
    "1. Add special tokens such as `CLS`, `SER`. \n",
    "2. Add padding to make texts if the same length (`max_length`). \n",
    "3. Create the attention mask. \n",
    "For the second process, I have mentioned in BiLSTM model. For the third precess, since we do the padding, there may have some `0` behind the short text. To focus on the text, mask is added to filter out the `0` part of tokenized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cdd4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f965f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokens and normalize them(padding, adding mask)\n",
    "def preprocessing(input_text, tokenizer):\n",
    "    token_text = tokenizer.encode_plus(input_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length = 30,\n",
    "                                       pad_to_max_length = True,\n",
    "                                       return_attention_mask = True,\n",
    "                                       return_tensors = 'pt')\n",
    "    return token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ba00c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in text:\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "emotion = torch.tensor(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1571fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of emotion: torch.Size([318936, 8])\n",
      "shape of token_id:  torch.Size([318936, 30])\n",
      "shape of attention_mask:  torch.Size([318936, 30])\n"
     ]
    }
   ],
   "source": [
    "print('shape of emotion:', emotion.shape)\n",
    "print('shape of token_id: ', token_id.shape)\n",
    "print('shape of attention_mask: ', attention_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15266800",
   "metadata": {},
   "source": [
    "#### Split train and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c603372",
   "metadata": {},
   "source": [
    "Here I use the index to do the splitting, then juxtapose the indexs to the `token_id` and `attention_mask`. After all preprocessing, I wrap them around a `DataLoader` object that is iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c88fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get splitting index\n",
    "train_idx, val_idx = train_test_split(np.arange(len(emotion)), test_size = 0.2, shuffle = True, stratify = emotion)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], attention_masks[train_idx], emotion[train_idx])\n",
    "val_set = TensorDataset(token_id[val_idx], attention_masks[val_idx], emotion[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(train_set, sampler = RandomSampler(train_set), batch_size = 32)\n",
    "validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20610d5c",
   "metadata": {},
   "source": [
    "### 3.2 Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d9c47",
   "metadata": {},
   "source": [
    "I use the pre-train bert model `bert-base-uncased` as the original model and Adam as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfe5226d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 8,\n",
    "                                                      output_attentions = False, output_hidden_states = False,)\n",
    "\n",
    "# set Adam optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, eps = 1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de691d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b9cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the device to gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856e728",
   "metadata": {},
   "source": [
    "### 3.3 Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a82fd",
   "metadata": {},
   "source": [
    "#### Define the training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49c86077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_metrics(preds, labels):\n",
    "    preds = np.argmax(preds, axis = 1).flatten()\n",
    "    labels = np.argmax(labels, axis = 1).flatten()\n",
    "    accuracy = accuracy_score(preds, labels)\n",
    "    precision = precision_score(preds, labels, average='macro')\n",
    "    recall = recall_score(preds, labels, average='macro')\n",
    "    f1 = f1_score(preds, labels, average='macro')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b7c1a",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c870078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                 | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.2695\n",
      "\t - Validation Accuracy: 0.5215\n",
      "\t - Validation Precision: 0.5156\n",
      "\t - Validation Recall: 0.5282\n",
      "\t - Validation f1: 0.4845\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 1/3 [07:43<15:27, 463.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.2291\n",
      "\t - Validation Accuracy: 0.5355\n",
      "\t - Validation Precision: 0.5285\n",
      "\t - Validation Recall: 0.5317\n",
      "\t - Validation f1: 0.4956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 2/3 [15:30<07:45, 465.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1949\n",
      "\t - Validation Accuracy: 0.5333\n",
      "\t - Validation Precision: 0.5258\n",
      "\t - Validation Recall: 0.5290\n",
      "\t - Validation f1: 0.4941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [23:16<00:00, 465.59s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for e in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    '''========== Training =========='''\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # send the data to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    '''========== Validation =========='''\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_f1 = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            eval_output = model(b_input_ids,\n",
    "                                token_type_ids = None,\n",
    "                                attention_mask = b_input_mask)\n",
    "        # record the logits\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_f1 = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update f1 only when (tn + fp) !=0; ignore nan\n",
    "        if b_f1 != 'nan': val_f1.append(b_f1)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation f1: {:.4f}\\n'.format(sum(val_f1)/len(val_f1)) if len(val_f1)>0 else '\\t - Validation f1: NaN')\n",
    "    file_path = './model/Bert_6_' + str(e)\n",
    "    torch.save(model, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9459ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, './model/Bert_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07e740",
   "metadata": {},
   "source": [
    "### 3.4 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eaec71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = torch.load('./model/Bert_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a889e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2213b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df_test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44f1faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the testing data\n",
    "test_token_id = []\n",
    "test_attention_mask = []\n",
    "\n",
    "for t in test_text:\n",
    "    encoding_dict = preprocessing(t, tokenizer)\n",
    "    test_token_id.append(encoding_dict['input_ids']) \n",
    "    test_attention_mask.append(encoding_dict['attention_mask'])\n",
    "    \n",
    "test_token_id = torch.cat(test_token_id, dim = 0)\n",
    "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "test_token_id = torch.unsqueeze(test_token_id, 1)\n",
    "test_attention_mask = torch.unsqueeze(test_attention_mask, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6efa2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: 0\n",
      "finished: 1000\n",
      "finished: 2000\n",
      "finished: 3000\n",
      "finished: 4000\n",
      "finished: 5000\n",
      "finished: 6000\n",
      "finished: 7000\n",
      "finished: 8000\n",
      "finished: 9000\n",
      "finished: 10000\n",
      "finished: 11000\n",
      "finished: 12000\n",
      "finished: 13000\n",
      "finished: 14000\n",
      "finished: 15000\n",
      "finished: 16000\n",
      "finished: 17000\n",
      "finished: 18000\n",
      "finished: 19000\n",
      "finished: 20000\n",
      "finished: 21000\n",
      "finished: 22000\n",
      "finished: 23000\n",
      "finished: 24000\n",
      "finished: 25000\n",
      "finished: 26000\n",
      "finished: 27000\n",
      "finished: 28000\n",
      "finished: 29000\n",
      "finished: 30000\n",
      "finished: 31000\n",
      "finished: 32000\n",
      "finished: 33000\n",
      "finished: 34000\n",
      "finished: 35000\n",
      "finished: 36000\n",
      "finished: 37000\n",
      "finished: 38000\n",
      "finished: 39000\n",
      "finished: 40000\n",
      "finished: 41000\n",
      "finished: 42000\n",
      "finished: 43000\n",
      "finished: 44000\n",
      "finished: 45000\n",
      "finished: 46000\n",
      "finished: 47000\n",
      "finished: 48000\n",
      "finished: 49000\n",
      "finished: 50000\n",
      "finished: 51000\n",
      "finished: 52000\n",
      "finished: 53000\n",
      "finished: 54000\n",
      "finished: 55000\n",
      "finished: 56000\n",
      "finished: 57000\n",
      "finished: 58000\n",
      "finished: 59000\n",
      "finished: 60000\n",
      "finished: 61000\n",
      "finished: 62000\n",
      "finished: 63000\n",
      "finished: 64000\n",
      "finished: 65000\n",
      "finished: 66000\n",
      "finished: 67000\n",
      "finished: 68000\n",
      "finished: 69000\n",
      "finished: 70000\n",
      "finished: 71000\n",
      "finished: 72000\n",
      "finished: 73000\n",
      "finished: 74000\n",
      "finished: 75000\n",
      "finished: 76000\n",
      "finished: 77000\n",
      "finished: 78000\n",
      "finished: 79000\n",
      "finished: 80000\n",
      "finished: 81000\n",
      "finished: 82000\n",
      "finished: 83000\n",
      "finished: 84000\n",
      "finished: 85000\n",
      "finished: 86000\n",
      "finished: 87000\n",
      "finished: 88000\n",
      "finished: 89000\n",
      "finished: 90000\n",
      "finished: 91000\n",
      "finished: 92000\n",
      "finished: 93000\n",
      "finished: 94000\n",
      "finished: 95000\n",
      "finished: 96000\n",
      "finished: 97000\n",
      "finished: 98000\n",
      "finished: 99000\n",
      "finished: 100000\n",
      "finished: 101000\n",
      "finished: 102000\n",
      "finished: 103000\n",
      "finished: 104000\n",
      "finished: 105000\n",
      "finished: 106000\n",
      "finished: 107000\n",
      "finished: 108000\n",
      "finished: 109000\n",
      "finished: 110000\n",
      "finished: 111000\n",
      "finished: 112000\n",
      "finished: 113000\n",
      "finished: 114000\n",
      "finished: 115000\n",
      "finished: 116000\n",
      "finished: 117000\n",
      "finished: 118000\n",
      "finished: 119000\n",
      "finished: 120000\n",
      "finished: 121000\n",
      "finished: 122000\n",
      "finished: 123000\n",
      "finished: 124000\n",
      "finished: 125000\n",
      "finished: 126000\n",
      "finished: 127000\n",
      "finished: 128000\n",
      "finished: 129000\n",
      "finished: 130000\n",
      "finished: 131000\n",
      "finished: 132000\n",
      "finished: 133000\n",
      "finished: 134000\n",
      "finished: 135000\n",
      "finished: 136000\n",
      "finished: 137000\n",
      "finished: 138000\n",
      "finished: 139000\n",
      "finished: 140000\n",
      "finished: 141000\n",
      "finished: 142000\n",
      "finished: 143000\n",
      "finished: 144000\n",
      "finished: 145000\n",
      "finished: 146000\n",
      "finished: 147000\n",
      "finished: 148000\n",
      "finished: 149000\n",
      "finished: 150000\n",
      "finished: 151000\n",
      "finished: 152000\n",
      "finished: 153000\n",
      "finished: 154000\n",
      "finished: 155000\n",
      "finished: 156000\n",
      "finished: 157000\n",
      "finished: 158000\n",
      "finished: 159000\n",
      "finished: 160000\n",
      "finished: 161000\n",
      "finished: 162000\n",
      "finished: 163000\n",
      "finished: 164000\n",
      "finished: 165000\n",
      "finished: 166000\n",
      "finished: 167000\n",
      "finished: 168000\n",
      "finished: 169000\n",
      "finished: 170000\n",
      "finished: 171000\n",
      "finished: 172000\n",
      "finished: 173000\n",
      "finished: 174000\n",
      "finished: 175000\n",
      "finished: 176000\n",
      "finished: 177000\n",
      "finished: 178000\n",
      "finished: 179000\n",
      "finished: 180000\n",
      "finished: 181000\n",
      "finished: 182000\n",
      "finished: 183000\n",
      "finished: 184000\n",
      "finished: 185000\n",
      "finished: 186000\n",
      "finished: 187000\n",
      "finished: 188000\n",
      "finished: 189000\n",
      "finished: 190000\n",
      "finished: 191000\n",
      "finished: 192000\n",
      "finished: 193000\n",
      "finished: 194000\n",
      "finished: 195000\n",
      "finished: 196000\n",
      "finished: 197000\n",
      "finished: 198000\n",
      "finished: 199000\n",
      "finished: 200000\n",
      "finished: 201000\n",
      "finished: 202000\n",
      "finished: 203000\n",
      "finished: 204000\n",
      "finished: 205000\n",
      "finished: 206000\n",
      "finished: 207000\n",
      "finished: 208000\n",
      "finished: 209000\n",
      "finished: 210000\n",
      "finished: 211000\n",
      "finished: 212000\n",
      "finished: 213000\n",
      "finished: 214000\n",
      "finished: 215000\n",
      "finished: 216000\n",
      "finished: 217000\n",
      "finished: 218000\n",
      "finished: 219000\n",
      "finished: 220000\n",
      "finished: 221000\n",
      "finished: 222000\n",
      "finished: 223000\n",
      "finished: 224000\n",
      "finished: 225000\n",
      "finished: 226000\n",
      "finished: 227000\n",
      "finished: 228000\n",
      "finished: 229000\n",
      "finished: 230000\n",
      "finished: 231000\n",
      "finished: 232000\n",
      "finished: 233000\n",
      "finished: 234000\n",
      "finished: 235000\n",
      "finished: 236000\n",
      "finished: 237000\n",
      "finished: 238000\n",
      "finished: 239000\n",
      "finished: 240000\n",
      "finished: 241000\n",
      "finished: 242000\n",
      "finished: 243000\n",
      "finished: 244000\n",
      "finished: 245000\n",
      "finished: 246000\n",
      "finished: 247000\n",
      "finished: 248000\n",
      "finished: 249000\n",
      "finished: 250000\n",
      "finished: 251000\n",
      "finished: 252000\n",
      "finished: 253000\n",
      "finished: 254000\n",
      "finished: 255000\n",
      "finished: 256000\n",
      "finished: 257000\n",
      "finished: 258000\n",
      "finished: 259000\n",
      "finished: 260000\n",
      "finished: 261000\n",
      "finished: 262000\n",
      "finished: 263000\n",
      "finished: 264000\n",
      "finished: 265000\n",
      "finished: 266000\n",
      "finished: 267000\n",
      "finished: 268000\n",
      "finished: 269000\n",
      "finished: 270000\n",
      "finished: 271000\n",
      "finished: 272000\n",
      "finished: 273000\n",
      "finished: 274000\n",
      "finished: 275000\n",
      "finished: 276000\n",
      "finished: 277000\n",
      "finished: 278000\n",
      "finished: 279000\n",
      "finished: 280000\n",
      "finished: 281000\n",
      "finished: 282000\n",
      "finished: 283000\n",
      "finished: 284000\n",
      "finished: 285000\n",
      "finished: 286000\n",
      "finished: 287000\n",
      "finished: 288000\n",
      "finished: 289000\n",
      "finished: 290000\n",
      "finished: 291000\n",
      "finished: 292000\n",
      "finished: 293000\n",
      "finished: 294000\n",
      "finished: 295000\n",
      "finished: 296000\n",
      "finished: 297000\n",
      "finished: 298000\n",
      "finished: 299000\n",
      "finished: 300000\n",
      "finished: 301000\n",
      "finished: 302000\n",
      "finished: 303000\n",
      "finished: 304000\n",
      "finished: 305000\n",
      "finished: 306000\n",
      "finished: 307000\n",
      "finished: 308000\n",
      "finished: 309000\n",
      "finished: 310000\n",
      "finished: 311000\n",
      "finished: 312000\n",
      "finished: 313000\n",
      "finished: 314000\n",
      "finished: 315000\n",
      "finished: 316000\n",
      "finished: 317000\n",
      "finished: 318000\n",
      "finished: 319000\n",
      "finished: 320000\n",
      "finished: 321000\n",
      "finished: 322000\n",
      "finished: 323000\n",
      "finished: 324000\n",
      "finished: 325000\n",
      "finished: 326000\n",
      "finished: 327000\n",
      "finished: 328000\n",
      "finished: 329000\n",
      "finished: 330000\n",
      "finished: 331000\n",
      "finished: 332000\n",
      "finished: 333000\n",
      "finished: 334000\n",
      "finished: 335000\n",
      "finished: 336000\n",
      "finished: 337000\n",
      "finished: 338000\n",
      "finished: 339000\n",
      "finished: 340000\n",
      "finished: 341000\n",
      "finished: 342000\n",
      "finished: 343000\n",
      "finished: 344000\n",
      "finished: 345000\n",
      "finished: 346000\n",
      "finished: 347000\n",
      "finished: 348000\n",
      "finished: 349000\n",
      "finished: 350000\n",
      "finished: 351000\n",
      "finished: 352000\n",
      "finished: 353000\n",
      "finished: 354000\n",
      "finished: 355000\n",
      "finished: 356000\n",
      "finished: 357000\n",
      "finished: 358000\n",
      "finished: 359000\n",
      "finished: 360000\n",
      "finished: 361000\n",
      "finished: 362000\n",
      "finished: 363000\n",
      "finished: 364000\n",
      "finished: 365000\n",
      "finished: 366000\n",
      "finished: 367000\n",
      "finished: 368000\n",
      "finished: 369000\n",
      "finished: 370000\n",
      "finished: 371000\n",
      "finished: 372000\n",
      "finished: 373000\n",
      "finished: 374000\n",
      "finished: 375000\n",
      "finished: 376000\n",
      "finished: 377000\n",
      "finished: 378000\n",
      "finished: 379000\n",
      "finished: 380000\n",
      "finished: 381000\n",
      "finished: 382000\n",
      "finished: 383000\n",
      "finished: 384000\n",
      "finished: 385000\n",
      "finished: 386000\n",
      "finished: 387000\n",
      "finished: 388000\n",
      "finished: 389000\n",
      "finished: 390000\n",
      "finished: 391000\n",
      "finished: 392000\n",
      "finished: 393000\n",
      "finished: 394000\n",
      "finished: 395000\n",
      "finished: 396000\n",
      "finished: 397000\n",
      "finished: 398000\n",
      "finished: 399000\n",
      "finished: 400000\n",
      "finished: 401000\n",
      "finished: 402000\n",
      "finished: 403000\n",
      "finished: 404000\n",
      "finished: 405000\n",
      "finished: 406000\n",
      "finished: 407000\n",
      "finished: 408000\n",
      "finished: 409000\n",
      "finished: 410000\n",
      "finished: 411000\n"
     ]
    }
   ],
   "source": [
    "# Forward pass, calculate logit predictions\n",
    "predict = []\n",
    "with torch.no_grad():\n",
    "    for t, m, i in zip(test_token_id, test_attention_mask, range(len(test_attention_mask))):\n",
    "        output = model(t.to(device), token_type_ids = None, attention_mask = m.to(device))\n",
    "        if i % 1000 == 0: print(\"finished:\", i)\n",
    "        predict.append(output.logits.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cca8055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = label_decode(label_encoder, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6168383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = df_test.tweet_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e70f58cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411972 411972\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet_id), len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12229b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction\n",
    "df = pd.DataFrame(list(zip(tweet_id, y_pred)), columns =['id', 'emotion']) \n",
    "df.to_csv(\"./result/result_Bert_6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30236b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a942ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d84a81",
   "metadata": {},
   "source": [
    "# RoBERTa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90827b31",
   "metadata": {},
   "source": [
    "Roberta is an improved model based on Bert. Let's see if it can has better performance.\n",
    "**Since the majority of process is similar to Bert Model mention in Part_2_3, here I'll just explain the parts don't mention before.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89600f55",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3102d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aba1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f06ceae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d26c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 20:33:55.763161: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 20:34:15.205786: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:/usr/local/cuda-11.4/lib64\n",
      "2022-11-22 20:34:15.206156: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:/usr/local/cuda-11.4/lib64\n",
      "2022-11-22 20:34:15.206171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 4.2.0\n",
      "tensorflow: 2.11.0\n",
      "keras: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import gensim\n",
    "import contractions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"gensim: \" + gensim.__version__)\n",
    "print(\"tensorflow: \" + tf.__version__)\n",
    "print(\"keras: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fd0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db911ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/Vivian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/Vivian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/Vivian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4a9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590417d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627d796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_pickle(\"part2_data/cleaned_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159dbf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, ``, add, me, on, #, Snapch...</td>\n",
       "      <td>People post `` add Snapchat '' must dehydrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@, brianklaas, As, we, see, ,, Trump, is, dan...</td>\n",
       "      <td>brianklaas see Trump dangerous freepress aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, &lt;, LH, &gt;]</td>\n",
       "      <td>ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "\n",
       "                                            unigrams  \\\n",
       "0  [People, who, post, ``, add, me, on, #, Snapch...   \n",
       "1  [@, brianklaas, As, we, see, ,, Trump, is, dan...   \n",
       "3    [Now, ISSA, is, stalking, Tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, <, LH, >]   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  People post `` add Snapchat '' must dehydrated...  \n",
       "1  brianklaas see Trump dangerous freepress aroun...  \n",
       "3                         ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.loc[df.identification == 'train']\n",
    "df_test = df.loc[df.identification == 'test']\n",
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a69df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "anger            39867\n",
      "anticipation    248935\n",
      "disgust         139101\n",
      "fear             63999\n",
      "joy             516017\n",
      "sadness         193437\n",
      "surprise         48729\n",
      "trust           205478\n",
      "Name: text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.groupby(['emotion']).count()['text'])\n",
    "# df_train['emotion'].value_counts().plot(kind='bar')\n",
    "# plt.title('Emotion class count')\n",
    "# plt.xlabel(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc34eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = df_train['emotion'] == 'anger'\n",
    "# df_select = df_train[condition].sample(39867)\n",
    "# condition = df_train['emotion'] == 'anticipation'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'disgust'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'fear'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'joy'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'sadness'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'surprise'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "# condition = df_train['emotion'] == 'trust'\n",
    "# df_tmp = df_train[condition].sample(39867)\n",
    "# df_select = pd.concat([df_select, df_tmp])\n",
    "\n",
    "# df_train = df_select.reset_index(drop=True)\n",
    "# print(df_train.groupby(['emotion']).count()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81638832",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a1380",
   "metadata": {},
   "source": [
    "### 2.1 Sentence preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8205ba",
   "metadata": {},
   "source": [
    "1. lemmatize: to uniform the text format\n",
    "2. remove stopword: since the negative terms may affect the emotion of the text, they should not be removed. Finally, I didn't remove the stopwords. The reason will be illustrated in the discussion part.\n",
    "3. add hashtags: here I take the hastags into consideration by adding them to the back of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf03de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()    # lemmatization\n",
    "    # keep the negative terms to maintian the semantic\n",
    "    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                        'even though', 'yet']\n",
    "    stop_words = [z for z in stop_words if z not in negative]\n",
    "    # preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words] #lemmatization\n",
    "    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split()]\n",
    "    return ' '.join([x for x in preprocessed_tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7744ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hashtags to text\n",
    "df_train['hashtags'] = df_train['hashtags'].apply(lambda x: ' '.join(map(str, x)))\n",
    "df_test['hashtags'] = df_test['hashtags'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"] + ' ' + df_train[\"hashtags\"]\n",
    "df_test[\"text\"] = df_test[\"text\"] + ' ' + df_test[\"hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2267e375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train[' text'] = df_train['text'].apply(lambda x: preprocess(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ff416ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train.text.values\n",
    "emotion = df_train.emotion.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162c608",
   "metadata": {},
   "source": [
    "### 2.2 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd943ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n"
     ]
    }
   ],
   "source": [
    "# deal with label into one hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion)\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be173e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = label_encode(label_encoder, emotion)\n",
    "emotion[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e38829",
   "metadata": {},
   "source": [
    "### 2.3 Tokenize encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2299fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrain tokenizer\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bb58754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokens and normalize them(padding, adding mask)\n",
    "def roberta_encode(input_text, tokenizer):\n",
    "    token_text = tokenizer.encode_plus(input_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length = 40,\n",
    "                                       pad_to_max_length = True,\n",
    "                                       return_attention_mask = True,\n",
    "                                       return_tensors = 'pt')\n",
    "    return token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e2656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in text:\n",
    "    encoding_dict = roberta_encode(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "emotion = torch.tensor(emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5a229",
   "metadata": {},
   "source": [
    "Since the parameterof the model is confirmed prviously (By training a lot of times). Here I don't set the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10de0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get splitting index\n",
    "train_idx, val_idx = train_test_split(np.arange(len(emotion)), test_size = 0.000008, shuffle = True, stratify = emotion, random_state = 42)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], attention_masks[train_idx], emotion[train_idx])\n",
    "val_set = TensorDataset(token_id[val_idx], attention_masks[val_idx], emotion[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(train_set, sampler = RandomSampler(train_set), batch_size = 32)\n",
    "validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdfebff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of emotion: torch.Size([1455563, 8])\n",
      "shape of token_id:  torch.Size([1455563, 40])\n",
      "shape of attention_mask:  torch.Size([1455563, 40])\n"
     ]
    }
   ],
   "source": [
    "print('shape of emotion:', emotion.shape)\n",
    "print('shape of token_id: ', token_id.shape)\n",
    "print('shape of attention_mask: ', attention_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c1bc8",
   "metadata": {},
   "source": [
    "## 3. RoBERTa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0a35d",
   "metadata": {},
   "source": [
    "### 3.1 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "                                                         num_labels = 8, output_attentions = False, \n",
    "                                                         output_hidden_states = False,\n",
    "                                                        ignore_mismatched_sizes=True)\n",
    "\n",
    "# set Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-5, eps = 1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69d96764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d8a8611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e44952",
   "metadata": {},
   "source": [
    "### 3.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e03e65",
   "metadata": {},
   "source": [
    "#### Define the training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eff4b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_metrics(preds, labels):\n",
    "    preds = np.argmax(preds, axis = 1).flatten()\n",
    "    labels = np.argmax(labels, axis = 1).flatten()\n",
    "    accuracy = accuracy_score(preds, labels)\n",
    "    precision = precision_score(preds, labels, average='macro')\n",
    "    recall = recall_score(preds, labels, average='macro')\n",
    "    f1 = f1_score(preds, labels, average='macro')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4924e",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "085a33ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1928\n",
      "\t - Validation Accuracy: 0.5000\n",
      "\t - Validation Precision: 0.3214\n",
      "\t - Validation Recall: 0.3214\n",
      "\t - Validation f1: 0.2976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [52:50<52:50, 3170.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1688\n",
      "\t - Validation Accuracy: 0.5833\n",
      "\t - Validation Precision: 0.5833\n",
      "\t - Validation Recall: 0.4556\n",
      "\t - Validation f1: 0.4905\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [1:35:52<00:00, 2876.40s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for e in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    '''========== Training =========='''\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    '''========== Validation =========='''\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_f1 = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            eval_output = model(b_input_ids,\n",
    "                                token_type_ids = None,\n",
    "                                attention_mask = b_input_mask)\n",
    "\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_f1 = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update f1 only when (tn + fp) !=0; ignore nan\n",
    "        if b_f1 != 'nan': val_f1.append(b_f1)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation f1: {:.4f}\\n'.format(sum(val_f1)/len(val_f1)) if len(val_f1)>0 else '\\t - Validation f1: NaN')\n",
    "    file_path = './model/RoBERTa_14_' + str(e)\n",
    "    torch.save(model, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d6dfc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model/RoBERTa_14')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945961b",
   "metadata": {},
   "source": [
    "### 3.3 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "657bcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/RoBERTa_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "376f241b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "562b5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df_test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "533a767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_id = []\n",
    "test_attention_mask = []\n",
    "\n",
    "for t in test_text:\n",
    "    encoding_dict = roberta_encode(t, tokenizer)\n",
    "    test_token_id.append(encoding_dict['input_ids']) \n",
    "    test_attention_mask.append(encoding_dict['attention_mask'])\n",
    "    \n",
    "test_token_id = torch.cat(test_token_id, dim = 0)\n",
    "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "test_token_id = torch.unsqueeze(test_token_id, 1)\n",
    "test_attention_mask = torch.unsqueeze(test_attention_mask, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1173c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: 0\n",
      "finished: 1000\n",
      "finished: 2000\n",
      "finished: 3000\n",
      "finished: 4000\n",
      "finished: 5000\n",
      "finished: 6000\n",
      "finished: 7000\n",
      "finished: 8000\n",
      "finished: 9000\n",
      "finished: 10000\n",
      "finished: 11000\n",
      "finished: 12000\n",
      "finished: 13000\n",
      "finished: 14000\n",
      "finished: 15000\n",
      "finished: 16000\n",
      "finished: 17000\n",
      "finished: 18000\n",
      "finished: 19000\n",
      "finished: 20000\n",
      "finished: 21000\n",
      "finished: 22000\n",
      "finished: 23000\n",
      "finished: 24000\n",
      "finished: 25000\n",
      "finished: 26000\n",
      "finished: 27000\n",
      "finished: 28000\n",
      "finished: 29000\n",
      "finished: 30000\n",
      "finished: 31000\n",
      "finished: 32000\n",
      "finished: 33000\n",
      "finished: 34000\n",
      "finished: 35000\n",
      "finished: 36000\n",
      "finished: 37000\n",
      "finished: 38000\n",
      "finished: 39000\n",
      "finished: 40000\n",
      "finished: 41000\n",
      "finished: 42000\n",
      "finished: 43000\n",
      "finished: 44000\n",
      "finished: 45000\n",
      "finished: 46000\n",
      "finished: 47000\n",
      "finished: 48000\n",
      "finished: 49000\n",
      "finished: 50000\n",
      "finished: 51000\n",
      "finished: 52000\n",
      "finished: 53000\n",
      "finished: 54000\n",
      "finished: 55000\n",
      "finished: 56000\n",
      "finished: 57000\n",
      "finished: 58000\n",
      "finished: 59000\n",
      "finished: 60000\n",
      "finished: 61000\n",
      "finished: 62000\n",
      "finished: 63000\n",
      "finished: 64000\n",
      "finished: 65000\n",
      "finished: 66000\n",
      "finished: 67000\n",
      "finished: 68000\n",
      "finished: 69000\n",
      "finished: 70000\n",
      "finished: 71000\n",
      "finished: 72000\n",
      "finished: 73000\n",
      "finished: 74000\n",
      "finished: 75000\n",
      "finished: 76000\n",
      "finished: 77000\n",
      "finished: 78000\n",
      "finished: 79000\n",
      "finished: 80000\n",
      "finished: 81000\n",
      "finished: 82000\n",
      "finished: 83000\n",
      "finished: 84000\n",
      "finished: 85000\n",
      "finished: 86000\n",
      "finished: 87000\n",
      "finished: 88000\n",
      "finished: 89000\n",
      "finished: 90000\n",
      "finished: 91000\n",
      "finished: 92000\n",
      "finished: 93000\n",
      "finished: 94000\n",
      "finished: 95000\n",
      "finished: 96000\n",
      "finished: 97000\n",
      "finished: 98000\n",
      "finished: 99000\n",
      "finished: 100000\n",
      "finished: 101000\n",
      "finished: 102000\n",
      "finished: 103000\n",
      "finished: 104000\n",
      "finished: 105000\n",
      "finished: 106000\n",
      "finished: 107000\n",
      "finished: 108000\n",
      "finished: 109000\n",
      "finished: 110000\n",
      "finished: 111000\n",
      "finished: 112000\n",
      "finished: 113000\n",
      "finished: 114000\n",
      "finished: 115000\n",
      "finished: 116000\n",
      "finished: 117000\n",
      "finished: 118000\n",
      "finished: 119000\n",
      "finished: 120000\n",
      "finished: 121000\n",
      "finished: 122000\n",
      "finished: 123000\n",
      "finished: 124000\n",
      "finished: 125000\n",
      "finished: 126000\n",
      "finished: 127000\n",
      "finished: 128000\n",
      "finished: 129000\n",
      "finished: 130000\n",
      "finished: 131000\n",
      "finished: 132000\n",
      "finished: 133000\n",
      "finished: 134000\n",
      "finished: 135000\n",
      "finished: 136000\n",
      "finished: 137000\n",
      "finished: 138000\n",
      "finished: 139000\n",
      "finished: 140000\n",
      "finished: 141000\n",
      "finished: 142000\n",
      "finished: 143000\n",
      "finished: 144000\n",
      "finished: 145000\n",
      "finished: 146000\n",
      "finished: 147000\n",
      "finished: 148000\n",
      "finished: 149000\n",
      "finished: 150000\n",
      "finished: 151000\n",
      "finished: 152000\n",
      "finished: 153000\n",
      "finished: 154000\n",
      "finished: 155000\n",
      "finished: 156000\n",
      "finished: 157000\n",
      "finished: 158000\n",
      "finished: 159000\n",
      "finished: 160000\n",
      "finished: 161000\n",
      "finished: 162000\n",
      "finished: 163000\n",
      "finished: 164000\n",
      "finished: 165000\n",
      "finished: 166000\n",
      "finished: 167000\n",
      "finished: 168000\n",
      "finished: 169000\n",
      "finished: 170000\n",
      "finished: 171000\n",
      "finished: 172000\n",
      "finished: 173000\n",
      "finished: 174000\n",
      "finished: 175000\n",
      "finished: 176000\n",
      "finished: 177000\n",
      "finished: 178000\n",
      "finished: 179000\n",
      "finished: 180000\n",
      "finished: 181000\n",
      "finished: 182000\n",
      "finished: 183000\n",
      "finished: 184000\n",
      "finished: 185000\n",
      "finished: 186000\n",
      "finished: 187000\n",
      "finished: 188000\n",
      "finished: 189000\n",
      "finished: 190000\n",
      "finished: 191000\n",
      "finished: 192000\n",
      "finished: 193000\n",
      "finished: 194000\n",
      "finished: 195000\n",
      "finished: 196000\n",
      "finished: 197000\n",
      "finished: 198000\n",
      "finished: 199000\n",
      "finished: 200000\n",
      "finished: 201000\n",
      "finished: 202000\n",
      "finished: 203000\n",
      "finished: 204000\n",
      "finished: 205000\n",
      "finished: 206000\n",
      "finished: 207000\n",
      "finished: 208000\n",
      "finished: 209000\n",
      "finished: 210000\n",
      "finished: 211000\n",
      "finished: 212000\n",
      "finished: 213000\n",
      "finished: 214000\n",
      "finished: 215000\n",
      "finished: 216000\n",
      "finished: 217000\n",
      "finished: 218000\n",
      "finished: 219000\n",
      "finished: 220000\n",
      "finished: 221000\n",
      "finished: 222000\n",
      "finished: 223000\n",
      "finished: 224000\n",
      "finished: 225000\n",
      "finished: 226000\n",
      "finished: 227000\n",
      "finished: 228000\n",
      "finished: 229000\n",
      "finished: 230000\n",
      "finished: 231000\n",
      "finished: 232000\n",
      "finished: 233000\n",
      "finished: 234000\n",
      "finished: 235000\n",
      "finished: 236000\n",
      "finished: 237000\n",
      "finished: 238000\n",
      "finished: 239000\n",
      "finished: 240000\n",
      "finished: 241000\n",
      "finished: 242000\n",
      "finished: 243000\n",
      "finished: 244000\n",
      "finished: 245000\n",
      "finished: 246000\n",
      "finished: 247000\n",
      "finished: 248000\n",
      "finished: 249000\n",
      "finished: 250000\n",
      "finished: 251000\n",
      "finished: 252000\n",
      "finished: 253000\n",
      "finished: 254000\n",
      "finished: 255000\n",
      "finished: 256000\n",
      "finished: 257000\n",
      "finished: 258000\n",
      "finished: 259000\n",
      "finished: 260000\n",
      "finished: 261000\n",
      "finished: 262000\n",
      "finished: 263000\n",
      "finished: 264000\n",
      "finished: 265000\n",
      "finished: 266000\n",
      "finished: 267000\n",
      "finished: 268000\n",
      "finished: 269000\n",
      "finished: 270000\n",
      "finished: 271000\n",
      "finished: 272000\n",
      "finished: 273000\n",
      "finished: 274000\n",
      "finished: 275000\n",
      "finished: 276000\n",
      "finished: 277000\n",
      "finished: 278000\n",
      "finished: 279000\n",
      "finished: 280000\n",
      "finished: 281000\n",
      "finished: 282000\n",
      "finished: 283000\n",
      "finished: 284000\n",
      "finished: 285000\n",
      "finished: 286000\n",
      "finished: 287000\n",
      "finished: 288000\n",
      "finished: 289000\n",
      "finished: 290000\n",
      "finished: 291000\n",
      "finished: 292000\n",
      "finished: 293000\n",
      "finished: 294000\n",
      "finished: 295000\n",
      "finished: 296000\n",
      "finished: 297000\n",
      "finished: 298000\n",
      "finished: 299000\n",
      "finished: 300000\n",
      "finished: 301000\n",
      "finished: 302000\n",
      "finished: 303000\n",
      "finished: 304000\n",
      "finished: 305000\n",
      "finished: 306000\n",
      "finished: 307000\n",
      "finished: 308000\n",
      "finished: 309000\n",
      "finished: 310000\n",
      "finished: 311000\n",
      "finished: 312000\n",
      "finished: 313000\n",
      "finished: 314000\n",
      "finished: 315000\n",
      "finished: 316000\n",
      "finished: 317000\n",
      "finished: 318000\n",
      "finished: 319000\n",
      "finished: 320000\n",
      "finished: 321000\n",
      "finished: 322000\n",
      "finished: 323000\n",
      "finished: 324000\n",
      "finished: 325000\n",
      "finished: 326000\n",
      "finished: 327000\n",
      "finished: 328000\n",
      "finished: 329000\n",
      "finished: 330000\n",
      "finished: 331000\n",
      "finished: 332000\n",
      "finished: 333000\n",
      "finished: 334000\n",
      "finished: 335000\n",
      "finished: 336000\n",
      "finished: 337000\n",
      "finished: 338000\n",
      "finished: 339000\n",
      "finished: 340000\n",
      "finished: 341000\n",
      "finished: 342000\n",
      "finished: 343000\n",
      "finished: 344000\n",
      "finished: 345000\n",
      "finished: 346000\n",
      "finished: 347000\n",
      "finished: 348000\n",
      "finished: 349000\n",
      "finished: 350000\n",
      "finished: 351000\n",
      "finished: 352000\n",
      "finished: 353000\n",
      "finished: 354000\n",
      "finished: 355000\n",
      "finished: 356000\n",
      "finished: 357000\n",
      "finished: 358000\n",
      "finished: 359000\n",
      "finished: 360000\n",
      "finished: 361000\n",
      "finished: 362000\n",
      "finished: 363000\n",
      "finished: 364000\n",
      "finished: 365000\n",
      "finished: 366000\n",
      "finished: 367000\n",
      "finished: 368000\n",
      "finished: 369000\n",
      "finished: 370000\n",
      "finished: 371000\n",
      "finished: 372000\n",
      "finished: 373000\n",
      "finished: 374000\n",
      "finished: 375000\n",
      "finished: 376000\n",
      "finished: 377000\n",
      "finished: 378000\n",
      "finished: 379000\n",
      "finished: 380000\n",
      "finished: 381000\n",
      "finished: 382000\n",
      "finished: 383000\n",
      "finished: 384000\n",
      "finished: 385000\n",
      "finished: 386000\n",
      "finished: 387000\n",
      "finished: 388000\n",
      "finished: 389000\n",
      "finished: 390000\n",
      "finished: 391000\n",
      "finished: 392000\n",
      "finished: 393000\n",
      "finished: 394000\n",
      "finished: 395000\n",
      "finished: 396000\n",
      "finished: 397000\n",
      "finished: 398000\n",
      "finished: 399000\n",
      "finished: 400000\n",
      "finished: 401000\n",
      "finished: 402000\n",
      "finished: 403000\n",
      "finished: 404000\n",
      "finished: 405000\n",
      "finished: 406000\n",
      "finished: 407000\n",
      "finished: 408000\n",
      "finished: 409000\n",
      "finished: 410000\n",
      "finished: 411000\n"
     ]
    }
   ],
   "source": [
    "# Forward pass, calculate logit predictions\n",
    "predict = []\n",
    "with torch.no_grad():\n",
    "    for t, m, i in zip(test_token_id, test_attention_mask, range(len(test_attention_mask))):\n",
    "        output = model(t.to(device), token_type_ids = None, attention_mask = m.to(device))\n",
    "        if i % 1000 == 0: print(\"finished:\", i)\n",
    "        predict.append(output.logits.cpu().numpy().flatten())\n",
    "        # predict.append(np.argmax(output.logits.cpu().numpy()).flatten().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af32f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = label_decode(label_encoder, predict)\n",
    "tweet_id = df_test.tweet_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf9ca90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(tweet_id, y_pred)), columns =['id', 'emotion']) \n",
    "df.to_csv(\"result_RoBERTa_14.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "61cb479dcbf5d74db36c2aa907f7fbf364fec9b5ac45c47d1234a4a4ff8e9cf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
